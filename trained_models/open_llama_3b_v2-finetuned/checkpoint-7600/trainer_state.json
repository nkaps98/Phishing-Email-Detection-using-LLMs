{
  "best_global_step": 7600,
  "best_metric": 1.63577139377594,
  "best_model_checkpoint": "trained_models/open_llama_3b_v2-finetuned/checkpoint-7600",
  "epoch": 1.9832985386221296,
  "eval_steps": 200,
  "global_step": 7600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026096033402922755,
      "grad_norm": 0.5674018263816833,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 3.0704,
      "step": 10
    },
    {
      "epoch": 0.005219206680584551,
      "grad_norm": 0.6218138337135315,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 3.0443,
      "step": 20
    },
    {
      "epoch": 0.007828810020876827,
      "grad_norm": 0.8268874883651733,
      "learning_rate": 8.999999999999999e-05,
      "loss": 3.2045,
      "step": 30
    },
    {
      "epoch": 0.010438413361169102,
      "grad_norm": 1.1403934955596924,
      "learning_rate": 0.00011999999999999999,
      "loss": 2.7954,
      "step": 40
    },
    {
      "epoch": 0.013048016701461378,
      "grad_norm": 0.9747563004493713,
      "learning_rate": 0.00015,
      "loss": 2.5438,
      "step": 50
    },
    {
      "epoch": 0.015657620041753653,
      "grad_norm": 1.1569613218307495,
      "learning_rate": 0.00017999999999999998,
      "loss": 2.4042,
      "step": 60
    },
    {
      "epoch": 0.01826722338204593,
      "grad_norm": 0.9749704599380493,
      "learning_rate": 0.00020999999999999998,
      "loss": 2.3695,
      "step": 70
    },
    {
      "epoch": 0.020876826722338204,
      "grad_norm": 0.7585469484329224,
      "learning_rate": 0.00023999999999999998,
      "loss": 2.3857,
      "step": 80
    },
    {
      "epoch": 0.02348643006263048,
      "grad_norm": 0.947637140750885,
      "learning_rate": 0.00027,
      "loss": 2.1834,
      "step": 90
    },
    {
      "epoch": 0.026096033402922755,
      "grad_norm": 0.9133210778236389,
      "learning_rate": 0.0003,
      "loss": 2.3061,
      "step": 100
    },
    {
      "epoch": 0.02870563674321503,
      "grad_norm": 0.9951289892196655,
      "learning_rate": 0.0002996033844526705,
      "loss": 2.3976,
      "step": 110
    },
    {
      "epoch": 0.031315240083507306,
      "grad_norm": 0.812362015247345,
      "learning_rate": 0.0002992067689053411,
      "loss": 2.2677,
      "step": 120
    },
    {
      "epoch": 0.03392484342379958,
      "grad_norm": 0.6869425177574158,
      "learning_rate": 0.0002988101533580116,
      "loss": 2.0828,
      "step": 130
    },
    {
      "epoch": 0.03653444676409186,
      "grad_norm": 0.9014551639556885,
      "learning_rate": 0.00029841353781068214,
      "loss": 2.1147,
      "step": 140
    },
    {
      "epoch": 0.03914405010438413,
      "grad_norm": 0.9272000193595886,
      "learning_rate": 0.00029801692226335267,
      "loss": 2.1517,
      "step": 150
    },
    {
      "epoch": 0.04175365344467641,
      "grad_norm": 1.0775706768035889,
      "learning_rate": 0.00029762030671602325,
      "loss": 2.0817,
      "step": 160
    },
    {
      "epoch": 0.044363256784968684,
      "grad_norm": 0.7524992227554321,
      "learning_rate": 0.0002972236911686938,
      "loss": 2.2415,
      "step": 170
    },
    {
      "epoch": 0.04697286012526096,
      "grad_norm": 0.9706153273582458,
      "learning_rate": 0.00029682707562136436,
      "loss": 2.0488,
      "step": 180
    },
    {
      "epoch": 0.049582463465553235,
      "grad_norm": 0.753613293170929,
      "learning_rate": 0.0002964304600740349,
      "loss": 2.1031,
      "step": 190
    },
    {
      "epoch": 0.05219206680584551,
      "grad_norm": 0.7258831858634949,
      "learning_rate": 0.0002960338445267054,
      "loss": 2.3184,
      "step": 200
    },
    {
      "epoch": 0.05219206680584551,
      "eval_loss": 2.158522605895996,
      "eval_runtime": 1.9511,
      "eval_samples_per_second": 61.502,
      "eval_steps_per_second": 7.688,
      "step": 200
    },
    {
      "epoch": 0.054801670146137786,
      "grad_norm": 0.6291121244430542,
      "learning_rate": 0.00029563722897937595,
      "loss": 2.0841,
      "step": 210
    },
    {
      "epoch": 0.05741127348643006,
      "grad_norm": 1.061065673828125,
      "learning_rate": 0.00029524061343204653,
      "loss": 1.9768,
      "step": 220
    },
    {
      "epoch": 0.06002087682672234,
      "grad_norm": 0.7952458262443542,
      "learning_rate": 0.00029484399788471706,
      "loss": 2.1954,
      "step": 230
    },
    {
      "epoch": 0.06263048016701461,
      "grad_norm": 0.8643245100975037,
      "learning_rate": 0.00029444738233738764,
      "loss": 2.2363,
      "step": 240
    },
    {
      "epoch": 0.0652400835073069,
      "grad_norm": 0.7564798593521118,
      "learning_rate": 0.00029405076679005817,
      "loss": 2.3283,
      "step": 250
    },
    {
      "epoch": 0.06784968684759916,
      "grad_norm": 0.9259117841720581,
      "learning_rate": 0.0002936541512427287,
      "loss": 2.2029,
      "step": 260
    },
    {
      "epoch": 0.07045929018789145,
      "grad_norm": 0.7411451935768127,
      "learning_rate": 0.00029325753569539923,
      "loss": 2.2365,
      "step": 270
    },
    {
      "epoch": 0.07306889352818371,
      "grad_norm": 0.8144798278808594,
      "learning_rate": 0.00029286092014806976,
      "loss": 1.9684,
      "step": 280
    },
    {
      "epoch": 0.075678496868476,
      "grad_norm": 0.7820281386375427,
      "learning_rate": 0.00029246430460074034,
      "loss": 2.0801,
      "step": 290
    },
    {
      "epoch": 0.07828810020876827,
      "grad_norm": 1.0555486679077148,
      "learning_rate": 0.00029206768905341087,
      "loss": 2.0416,
      "step": 300
    },
    {
      "epoch": 0.08089770354906055,
      "grad_norm": 0.8850928544998169,
      "learning_rate": 0.0002916710735060814,
      "loss": 2.2009,
      "step": 310
    },
    {
      "epoch": 0.08350730688935282,
      "grad_norm": 0.9382954835891724,
      "learning_rate": 0.0002912744579587519,
      "loss": 2.087,
      "step": 320
    },
    {
      "epoch": 0.0861169102296451,
      "grad_norm": 0.9085153341293335,
      "learning_rate": 0.0002908778424114225,
      "loss": 2.1131,
      "step": 330
    },
    {
      "epoch": 0.08872651356993737,
      "grad_norm": 0.8868969082832336,
      "learning_rate": 0.00029048122686409304,
      "loss": 2.2102,
      "step": 340
    },
    {
      "epoch": 0.09133611691022965,
      "grad_norm": 0.9636728167533875,
      "learning_rate": 0.0002900846113167636,
      "loss": 2.1887,
      "step": 350
    },
    {
      "epoch": 0.09394572025052192,
      "grad_norm": 0.9404574036598206,
      "learning_rate": 0.00028968799576943415,
      "loss": 2.1067,
      "step": 360
    },
    {
      "epoch": 0.0965553235908142,
      "grad_norm": 0.7583953142166138,
      "learning_rate": 0.0002892913802221047,
      "loss": 2.0551,
      "step": 370
    },
    {
      "epoch": 0.09916492693110647,
      "grad_norm": 1.1006762981414795,
      "learning_rate": 0.0002888947646747752,
      "loss": 2.0574,
      "step": 380
    },
    {
      "epoch": 0.10177453027139875,
      "grad_norm": 0.8325783014297485,
      "learning_rate": 0.00028849814912744573,
      "loss": 1.9603,
      "step": 390
    },
    {
      "epoch": 0.10438413361169102,
      "grad_norm": 0.8022867441177368,
      "learning_rate": 0.0002881015335801163,
      "loss": 2.1703,
      "step": 400
    },
    {
      "epoch": 0.10438413361169102,
      "eval_loss": 2.0712902545928955,
      "eval_runtime": 1.947,
      "eval_samples_per_second": 61.634,
      "eval_steps_per_second": 7.704,
      "step": 400
    },
    {
      "epoch": 0.1069937369519833,
      "grad_norm": 1.0642465353012085,
      "learning_rate": 0.00028770491803278684,
      "loss": 1.9728,
      "step": 410
    },
    {
      "epoch": 0.10960334029227557,
      "grad_norm": 0.8343459367752075,
      "learning_rate": 0.0002873083024854574,
      "loss": 2.0274,
      "step": 420
    },
    {
      "epoch": 0.11221294363256785,
      "grad_norm": 1.091658592224121,
      "learning_rate": 0.00028691168693812795,
      "loss": 2.0097,
      "step": 430
    },
    {
      "epoch": 0.11482254697286012,
      "grad_norm": 0.9619917869567871,
      "learning_rate": 0.0002865150713907985,
      "loss": 2.0294,
      "step": 440
    },
    {
      "epoch": 0.1174321503131524,
      "grad_norm": 0.8897931575775146,
      "learning_rate": 0.000286118455843469,
      "loss": 2.04,
      "step": 450
    },
    {
      "epoch": 0.12004175365344467,
      "grad_norm": 0.8020730018615723,
      "learning_rate": 0.0002857218402961396,
      "loss": 2.0709,
      "step": 460
    },
    {
      "epoch": 0.12265135699373696,
      "grad_norm": 0.8248649835586548,
      "learning_rate": 0.0002853252247488101,
      "loss": 2.0773,
      "step": 470
    },
    {
      "epoch": 0.12526096033402923,
      "grad_norm": 0.9192555546760559,
      "learning_rate": 0.0002849286092014807,
      "loss": 2.0476,
      "step": 480
    },
    {
      "epoch": 0.1278705636743215,
      "grad_norm": 0.9737438559532166,
      "learning_rate": 0.00028453199365415123,
      "loss": 2.1772,
      "step": 490
    },
    {
      "epoch": 0.1304801670146138,
      "grad_norm": 0.9173155426979065,
      "learning_rate": 0.00028413537810682176,
      "loss": 2.0229,
      "step": 500
    },
    {
      "epoch": 0.13308977035490605,
      "grad_norm": 0.8083041310310364,
      "learning_rate": 0.0002837387625594923,
      "loss": 2.0473,
      "step": 510
    },
    {
      "epoch": 0.13569937369519833,
      "grad_norm": 0.843513548374176,
      "learning_rate": 0.00028334214701216287,
      "loss": 2.1368,
      "step": 520
    },
    {
      "epoch": 0.1383089770354906,
      "grad_norm": 1.1508797407150269,
      "learning_rate": 0.0002829455314648334,
      "loss": 2.1076,
      "step": 530
    },
    {
      "epoch": 0.1409185803757829,
      "grad_norm": 0.8230304718017578,
      "learning_rate": 0.00028254891591750393,
      "loss": 2.0696,
      "step": 540
    },
    {
      "epoch": 0.14352818371607515,
      "grad_norm": 0.8754382729530334,
      "learning_rate": 0.0002821523003701745,
      "loss": 1.935,
      "step": 550
    },
    {
      "epoch": 0.14613778705636743,
      "grad_norm": 1.0004335641860962,
      "learning_rate": 0.00028175568482284504,
      "loss": 1.9767,
      "step": 560
    },
    {
      "epoch": 0.1487473903966597,
      "grad_norm": 0.9675282835960388,
      "learning_rate": 0.00028135906927551557,
      "loss": 2.2406,
      "step": 570
    },
    {
      "epoch": 0.151356993736952,
      "grad_norm": 0.8060902953147888,
      "learning_rate": 0.0002809624537281861,
      "loss": 2.0761,
      "step": 580
    },
    {
      "epoch": 0.15396659707724425,
      "grad_norm": 0.7250822186470032,
      "learning_rate": 0.0002805658381808567,
      "loss": 2.1825,
      "step": 590
    },
    {
      "epoch": 0.15657620041753653,
      "grad_norm": 0.7907674312591553,
      "learning_rate": 0.0002801692226335272,
      "loss": 1.982,
      "step": 600
    },
    {
      "epoch": 0.15657620041753653,
      "eval_loss": 2.018903970718384,
      "eval_runtime": 1.9583,
      "eval_samples_per_second": 61.277,
      "eval_steps_per_second": 7.66,
      "step": 600
    },
    {
      "epoch": 0.15918580375782881,
      "grad_norm": 0.8435476422309875,
      "learning_rate": 0.0002797726070861978,
      "loss": 2.1251,
      "step": 610
    },
    {
      "epoch": 0.1617954070981211,
      "grad_norm": 0.8854535818099976,
      "learning_rate": 0.0002793759915388683,
      "loss": 2.1133,
      "step": 620
    },
    {
      "epoch": 0.16440501043841335,
      "grad_norm": 0.9019836187362671,
      "learning_rate": 0.00027897937599153885,
      "loss": 2.0187,
      "step": 630
    },
    {
      "epoch": 0.16701461377870563,
      "grad_norm": 0.7847862839698792,
      "learning_rate": 0.0002785827604442094,
      "loss": 1.8023,
      "step": 640
    },
    {
      "epoch": 0.16962421711899792,
      "grad_norm": 0.9369961619377136,
      "learning_rate": 0.00027818614489687996,
      "loss": 1.9976,
      "step": 650
    },
    {
      "epoch": 0.1722338204592902,
      "grad_norm": 0.8293300867080688,
      "learning_rate": 0.0002777895293495505,
      "loss": 2.0499,
      "step": 660
    },
    {
      "epoch": 0.17484342379958245,
      "grad_norm": 0.9406370520591736,
      "learning_rate": 0.000277392913802221,
      "loss": 2.0682,
      "step": 670
    },
    {
      "epoch": 0.17745302713987474,
      "grad_norm": 0.9240376353263855,
      "learning_rate": 0.00027699629825489154,
      "loss": 2.1029,
      "step": 680
    },
    {
      "epoch": 0.18006263048016702,
      "grad_norm": 1.0967044830322266,
      "learning_rate": 0.00027659968270756207,
      "loss": 2.0127,
      "step": 690
    },
    {
      "epoch": 0.1826722338204593,
      "grad_norm": 0.8507778644561768,
      "learning_rate": 0.00027620306716023265,
      "loss": 2.1006,
      "step": 700
    },
    {
      "epoch": 0.18528183716075156,
      "grad_norm": 0.8718045949935913,
      "learning_rate": 0.0002758064516129032,
      "loss": 1.9042,
      "step": 710
    },
    {
      "epoch": 0.18789144050104384,
      "grad_norm": 0.9817553758621216,
      "learning_rate": 0.00027540983606557377,
      "loss": 2.0113,
      "step": 720
    },
    {
      "epoch": 0.19050104384133612,
      "grad_norm": 0.947382390499115,
      "learning_rate": 0.0002750132205182443,
      "loss": 1.9347,
      "step": 730
    },
    {
      "epoch": 0.1931106471816284,
      "grad_norm": 0.8064536452293396,
      "learning_rate": 0.0002746166049709148,
      "loss": 1.7812,
      "step": 740
    },
    {
      "epoch": 0.19572025052192066,
      "grad_norm": 0.9016521573066711,
      "learning_rate": 0.00027421998942358535,
      "loss": 2.0169,
      "step": 750
    },
    {
      "epoch": 0.19832985386221294,
      "grad_norm": 0.9260363578796387,
      "learning_rate": 0.00027382337387625593,
      "loss": 2.052,
      "step": 760
    },
    {
      "epoch": 0.20093945720250522,
      "grad_norm": 0.9146727323532104,
      "learning_rate": 0.00027342675832892646,
      "loss": 1.9946,
      "step": 770
    },
    {
      "epoch": 0.2035490605427975,
      "grad_norm": 0.9660024046897888,
      "learning_rate": 0.00027303014278159704,
      "loss": 1.9744,
      "step": 780
    },
    {
      "epoch": 0.20615866388308976,
      "grad_norm": 0.9407098889350891,
      "learning_rate": 0.0002726335272342676,
      "loss": 1.8913,
      "step": 790
    },
    {
      "epoch": 0.20876826722338204,
      "grad_norm": 1.0909687280654907,
      "learning_rate": 0.0002722369116869381,
      "loss": 1.9201,
      "step": 800
    },
    {
      "epoch": 0.20876826722338204,
      "eval_loss": 1.9693976640701294,
      "eval_runtime": 1.9562,
      "eval_samples_per_second": 61.345,
      "eval_steps_per_second": 7.668,
      "step": 800
    },
    {
      "epoch": 0.21137787056367432,
      "grad_norm": 0.9845883846282959,
      "learning_rate": 0.00027184029613960863,
      "loss": 1.7175,
      "step": 810
    },
    {
      "epoch": 0.2139874739039666,
      "grad_norm": 1.2494102716445923,
      "learning_rate": 0.00027144368059227916,
      "loss": 2.0363,
      "step": 820
    },
    {
      "epoch": 0.21659707724425886,
      "grad_norm": 0.8696415424346924,
      "learning_rate": 0.00027104706504494974,
      "loss": 2.07,
      "step": 830
    },
    {
      "epoch": 0.21920668058455114,
      "grad_norm": 1.9038904905319214,
      "learning_rate": 0.00027065044949762027,
      "loss": 1.9979,
      "step": 840
    },
    {
      "epoch": 0.22181628392484343,
      "grad_norm": 0.9960935711860657,
      "learning_rate": 0.00027025383395029085,
      "loss": 1.8428,
      "step": 850
    },
    {
      "epoch": 0.2244258872651357,
      "grad_norm": 0.9414791464805603,
      "learning_rate": 0.0002698572184029614,
      "loss": 2.1733,
      "step": 860
    },
    {
      "epoch": 0.22703549060542796,
      "grad_norm": 0.8800733089447021,
      "learning_rate": 0.0002694606028556319,
      "loss": 1.8361,
      "step": 870
    },
    {
      "epoch": 0.22964509394572025,
      "grad_norm": 1.0521997213363647,
      "learning_rate": 0.00026906398730830244,
      "loss": 1.9347,
      "step": 880
    },
    {
      "epoch": 0.23225469728601253,
      "grad_norm": 0.7658447623252869,
      "learning_rate": 0.000268667371760973,
      "loss": 1.9968,
      "step": 890
    },
    {
      "epoch": 0.2348643006263048,
      "grad_norm": 0.8137977719306946,
      "learning_rate": 0.00026827075621364355,
      "loss": 2.0052,
      "step": 900
    },
    {
      "epoch": 0.23747390396659707,
      "grad_norm": 0.8816063404083252,
      "learning_rate": 0.00026787414066631413,
      "loss": 2.0743,
      "step": 910
    },
    {
      "epoch": 0.24008350730688935,
      "grad_norm": 0.8368040323257446,
      "learning_rate": 0.00026747752511898466,
      "loss": 2.0207,
      "step": 920
    },
    {
      "epoch": 0.24269311064718163,
      "grad_norm": 0.7595033049583435,
      "learning_rate": 0.0002670809095716552,
      "loss": 1.7798,
      "step": 930
    },
    {
      "epoch": 0.2453027139874739,
      "grad_norm": 0.9625061750411987,
      "learning_rate": 0.0002666842940243257,
      "loss": 1.8725,
      "step": 940
    },
    {
      "epoch": 0.24791231732776617,
      "grad_norm": 0.8767263293266296,
      "learning_rate": 0.0002662876784769963,
      "loss": 1.882,
      "step": 950
    },
    {
      "epoch": 0.25052192066805845,
      "grad_norm": 0.9977782368659973,
      "learning_rate": 0.00026589106292966683,
      "loss": 2.1181,
      "step": 960
    },
    {
      "epoch": 0.2531315240083507,
      "grad_norm": 0.9149383902549744,
      "learning_rate": 0.00026549444738233736,
      "loss": 2.0657,
      "step": 970
    },
    {
      "epoch": 0.255741127348643,
      "grad_norm": 1.035982608795166,
      "learning_rate": 0.00026509783183500794,
      "loss": 1.9367,
      "step": 980
    },
    {
      "epoch": 0.25835073068893527,
      "grad_norm": 0.8649868369102478,
      "learning_rate": 0.00026470121628767847,
      "loss": 1.8925,
      "step": 990
    },
    {
      "epoch": 0.2609603340292276,
      "grad_norm": 0.9477616548538208,
      "learning_rate": 0.000264304600740349,
      "loss": 1.9215,
      "step": 1000
    },
    {
      "epoch": 0.2609603340292276,
      "eval_loss": 1.9352258443832397,
      "eval_runtime": 1.9504,
      "eval_samples_per_second": 61.526,
      "eval_steps_per_second": 7.691,
      "step": 1000
    },
    {
      "epoch": 0.26356993736951984,
      "grad_norm": 0.9826905727386475,
      "learning_rate": 0.0002639079851930195,
      "loss": 1.9625,
      "step": 1010
    },
    {
      "epoch": 0.2661795407098121,
      "grad_norm": 0.8566059470176697,
      "learning_rate": 0.0002635113696456901,
      "loss": 1.9082,
      "step": 1020
    },
    {
      "epoch": 0.2687891440501044,
      "grad_norm": 0.9774667620658875,
      "learning_rate": 0.00026311475409836063,
      "loss": 1.9423,
      "step": 1030
    },
    {
      "epoch": 0.27139874739039666,
      "grad_norm": 0.7687703967094421,
      "learning_rate": 0.00026271813855103116,
      "loss": 2.0618,
      "step": 1040
    },
    {
      "epoch": 0.2740083507306889,
      "grad_norm": 0.9483172297477722,
      "learning_rate": 0.0002623215230037017,
      "loss": 1.8534,
      "step": 1050
    },
    {
      "epoch": 0.2766179540709812,
      "grad_norm": 1.062978744506836,
      "learning_rate": 0.0002619249074563723,
      "loss": 1.7471,
      "step": 1060
    },
    {
      "epoch": 0.2792275574112735,
      "grad_norm": 0.9602171182632446,
      "learning_rate": 0.0002615282919090428,
      "loss": 2.1339,
      "step": 1070
    },
    {
      "epoch": 0.2818371607515658,
      "grad_norm": 0.886789083480835,
      "learning_rate": 0.0002611316763617134,
      "loss": 1.9409,
      "step": 1080
    },
    {
      "epoch": 0.28444676409185804,
      "grad_norm": 1.1317965984344482,
      "learning_rate": 0.0002607350608143839,
      "loss": 1.7789,
      "step": 1090
    },
    {
      "epoch": 0.2870563674321503,
      "grad_norm": 1.4251809120178223,
      "learning_rate": 0.00026033844526705444,
      "loss": 2.1073,
      "step": 1100
    },
    {
      "epoch": 0.2896659707724426,
      "grad_norm": 0.9326468706130981,
      "learning_rate": 0.00025994182971972497,
      "loss": 2.0399,
      "step": 1110
    },
    {
      "epoch": 0.29227557411273486,
      "grad_norm": 1.1513912677764893,
      "learning_rate": 0.0002595452141723955,
      "loss": 2.136,
      "step": 1120
    },
    {
      "epoch": 0.2948851774530271,
      "grad_norm": 1.1223677396774292,
      "learning_rate": 0.0002591485986250661,
      "loss": 1.9307,
      "step": 1130
    },
    {
      "epoch": 0.2974947807933194,
      "grad_norm": 1.0658546686172485,
      "learning_rate": 0.0002587519830777366,
      "loss": 2.0076,
      "step": 1140
    },
    {
      "epoch": 0.3001043841336117,
      "grad_norm": 0.8869522213935852,
      "learning_rate": 0.0002583553675304072,
      "loss": 2.0377,
      "step": 1150
    },
    {
      "epoch": 0.302713987473904,
      "grad_norm": 1.0145838260650635,
      "learning_rate": 0.0002579587519830777,
      "loss": 1.8301,
      "step": 1160
    },
    {
      "epoch": 0.30532359081419624,
      "grad_norm": 0.8808870315551758,
      "learning_rate": 0.00025756213643574825,
      "loss": 1.8711,
      "step": 1170
    },
    {
      "epoch": 0.3079331941544885,
      "grad_norm": 0.8661899566650391,
      "learning_rate": 0.0002571655208884188,
      "loss": 1.957,
      "step": 1180
    },
    {
      "epoch": 0.3105427974947808,
      "grad_norm": 0.9800334572792053,
      "learning_rate": 0.00025676890534108936,
      "loss": 2.228,
      "step": 1190
    },
    {
      "epoch": 0.31315240083507306,
      "grad_norm": 1.0471357107162476,
      "learning_rate": 0.0002563722897937599,
      "loss": 2.0418,
      "step": 1200
    },
    {
      "epoch": 0.31315240083507306,
      "eval_loss": 1.9177932739257812,
      "eval_runtime": 1.9504,
      "eval_samples_per_second": 61.527,
      "eval_steps_per_second": 7.691,
      "step": 1200
    },
    {
      "epoch": 0.3157620041753653,
      "grad_norm": 1.0487757921218872,
      "learning_rate": 0.00025597567424643047,
      "loss": 1.9799,
      "step": 1210
    },
    {
      "epoch": 0.31837160751565763,
      "grad_norm": 0.8868105411529541,
      "learning_rate": 0.000255579058699101,
      "loss": 2.0601,
      "step": 1220
    },
    {
      "epoch": 0.3209812108559499,
      "grad_norm": 0.9345106482505798,
      "learning_rate": 0.00025518244315177153,
      "loss": 1.8882,
      "step": 1230
    },
    {
      "epoch": 0.3235908141962422,
      "grad_norm": 0.9802959561347961,
      "learning_rate": 0.00025478582760444206,
      "loss": 1.8784,
      "step": 1240
    },
    {
      "epoch": 0.32620041753653445,
      "grad_norm": 1.0295313596725464,
      "learning_rate": 0.0002543892120571126,
      "loss": 2.0216,
      "step": 1250
    },
    {
      "epoch": 0.3288100208768267,
      "grad_norm": 0.9110903143882751,
      "learning_rate": 0.00025399259650978317,
      "loss": 1.9524,
      "step": 1260
    },
    {
      "epoch": 0.331419624217119,
      "grad_norm": 0.9647563099861145,
      "learning_rate": 0.0002535959809624537,
      "loss": 1.9544,
      "step": 1270
    },
    {
      "epoch": 0.33402922755741127,
      "grad_norm": 1.1721638441085815,
      "learning_rate": 0.0002531993654151243,
      "loss": 1.877,
      "step": 1280
    },
    {
      "epoch": 0.3366388308977035,
      "grad_norm": 0.9151224493980408,
      "learning_rate": 0.0002528027498677948,
      "loss": 2.0299,
      "step": 1290
    },
    {
      "epoch": 0.33924843423799583,
      "grad_norm": 1.4512884616851807,
      "learning_rate": 0.00025240613432046534,
      "loss": 2.0695,
      "step": 1300
    },
    {
      "epoch": 0.3418580375782881,
      "grad_norm": 0.9873974919319153,
      "learning_rate": 0.00025200951877313586,
      "loss": 1.6184,
      "step": 1310
    },
    {
      "epoch": 0.3444676409185804,
      "grad_norm": 1.0201534032821655,
      "learning_rate": 0.00025161290322580645,
      "loss": 1.8942,
      "step": 1320
    },
    {
      "epoch": 0.34707724425887265,
      "grad_norm": 0.9975689053535461,
      "learning_rate": 0.000251216287678477,
      "loss": 1.9776,
      "step": 1330
    },
    {
      "epoch": 0.3496868475991649,
      "grad_norm": 1.4396620988845825,
      "learning_rate": 0.00025081967213114756,
      "loss": 1.9603,
      "step": 1340
    },
    {
      "epoch": 0.3522964509394572,
      "grad_norm": 0.9855846166610718,
      "learning_rate": 0.0002504230565838181,
      "loss": 1.9315,
      "step": 1350
    },
    {
      "epoch": 0.35490605427974947,
      "grad_norm": 0.9020324349403381,
      "learning_rate": 0.0002500264410364886,
      "loss": 1.8466,
      "step": 1360
    },
    {
      "epoch": 0.3575156576200417,
      "grad_norm": 1.2273333072662354,
      "learning_rate": 0.00024962982548915914,
      "loss": 1.8509,
      "step": 1370
    },
    {
      "epoch": 0.36012526096033404,
      "grad_norm": 0.9154700636863708,
      "learning_rate": 0.00024923320994182967,
      "loss": 1.8704,
      "step": 1380
    },
    {
      "epoch": 0.3627348643006263,
      "grad_norm": 0.930195152759552,
      "learning_rate": 0.00024883659439450025,
      "loss": 1.9295,
      "step": 1390
    },
    {
      "epoch": 0.3653444676409186,
      "grad_norm": 0.9869967699050903,
      "learning_rate": 0.0002484399788471708,
      "loss": 1.9145,
      "step": 1400
    },
    {
      "epoch": 0.3653444676409186,
      "eval_loss": 1.8837230205535889,
      "eval_runtime": 1.9555,
      "eval_samples_per_second": 61.365,
      "eval_steps_per_second": 7.671,
      "step": 1400
    },
    {
      "epoch": 0.36795407098121086,
      "grad_norm": 1.0864272117614746,
      "learning_rate": 0.0002480433632998413,
      "loss": 1.9385,
      "step": 1410
    },
    {
      "epoch": 0.3705636743215031,
      "grad_norm": 1.1286914348602295,
      "learning_rate": 0.00024764674775251184,
      "loss": 1.9843,
      "step": 1420
    },
    {
      "epoch": 0.3731732776617954,
      "grad_norm": 1.0476794242858887,
      "learning_rate": 0.0002472501322051824,
      "loss": 1.8409,
      "step": 1430
    },
    {
      "epoch": 0.3757828810020877,
      "grad_norm": 1.1954210996627808,
      "learning_rate": 0.00024685351665785295,
      "loss": 2.1954,
      "step": 1440
    },
    {
      "epoch": 0.37839248434237993,
      "grad_norm": 1.0591202974319458,
      "learning_rate": 0.00024645690111052353,
      "loss": 1.8675,
      "step": 1450
    },
    {
      "epoch": 0.38100208768267224,
      "grad_norm": 0.9339403510093689,
      "learning_rate": 0.00024606028556319406,
      "loss": 1.8016,
      "step": 1460
    },
    {
      "epoch": 0.3836116910229645,
      "grad_norm": 0.9407877922058105,
      "learning_rate": 0.0002456636700158646,
      "loss": 1.7932,
      "step": 1470
    },
    {
      "epoch": 0.3862212943632568,
      "grad_norm": 0.9935060143470764,
      "learning_rate": 0.0002452670544685351,
      "loss": 1.8995,
      "step": 1480
    },
    {
      "epoch": 0.38883089770354906,
      "grad_norm": 1.1277711391448975,
      "learning_rate": 0.0002448704389212057,
      "loss": 1.9393,
      "step": 1490
    },
    {
      "epoch": 0.3914405010438413,
      "grad_norm": 1.2970212697982788,
      "learning_rate": 0.00024447382337387623,
      "loss": 1.7435,
      "step": 1500
    },
    {
      "epoch": 0.3940501043841336,
      "grad_norm": 1.2563313245773315,
      "learning_rate": 0.00024407720782654679,
      "loss": 1.8226,
      "step": 1510
    },
    {
      "epoch": 0.3966597077244259,
      "grad_norm": 0.9439262747764587,
      "learning_rate": 0.00024368059227921731,
      "loss": 2.1483,
      "step": 1520
    },
    {
      "epoch": 0.39926931106471814,
      "grad_norm": 1.4578781127929688,
      "learning_rate": 0.00024328397673188784,
      "loss": 1.8204,
      "step": 1530
    },
    {
      "epoch": 0.40187891440501045,
      "grad_norm": 1.074482798576355,
      "learning_rate": 0.00024288736118455842,
      "loss": 1.6698,
      "step": 1540
    },
    {
      "epoch": 0.4044885177453027,
      "grad_norm": 1.1853457689285278,
      "learning_rate": 0.00024249074563722895,
      "loss": 2.0743,
      "step": 1550
    },
    {
      "epoch": 0.407098121085595,
      "grad_norm": 1.3599361181259155,
      "learning_rate": 0.0002420941300898995,
      "loss": 1.7943,
      "step": 1560
    },
    {
      "epoch": 0.40970772442588727,
      "grad_norm": 1.078812599182129,
      "learning_rate": 0.00024169751454257004,
      "loss": 2.0082,
      "step": 1570
    },
    {
      "epoch": 0.4123173277661795,
      "grad_norm": 0.880137026309967,
      "learning_rate": 0.0002413008989952406,
      "loss": 1.9783,
      "step": 1580
    },
    {
      "epoch": 0.41492693110647183,
      "grad_norm": 0.9787107706069946,
      "learning_rate": 0.00024090428344791112,
      "loss": 1.7999,
      "step": 1590
    },
    {
      "epoch": 0.4175365344467641,
      "grad_norm": 1.1105314493179321,
      "learning_rate": 0.0002405076679005817,
      "loss": 1.836,
      "step": 1600
    },
    {
      "epoch": 0.4175365344467641,
      "eval_loss": 1.8578822612762451,
      "eval_runtime": 1.9632,
      "eval_samples_per_second": 61.124,
      "eval_steps_per_second": 7.64,
      "step": 1600
    },
    {
      "epoch": 0.4201461377870564,
      "grad_norm": 1.2280937433242798,
      "learning_rate": 0.00024011105235325223,
      "loss": 1.8892,
      "step": 1610
    },
    {
      "epoch": 0.42275574112734865,
      "grad_norm": 1.1123913526535034,
      "learning_rate": 0.0002397144368059228,
      "loss": 1.7381,
      "step": 1620
    },
    {
      "epoch": 0.4253653444676409,
      "grad_norm": 1.2090628147125244,
      "learning_rate": 0.00023931782125859332,
      "loss": 2.001,
      "step": 1630
    },
    {
      "epoch": 0.4279749478079332,
      "grad_norm": 0.9430815577507019,
      "learning_rate": 0.00023892120571126387,
      "loss": 2.122,
      "step": 1640
    },
    {
      "epoch": 0.43058455114822547,
      "grad_norm": 1.15635085105896,
      "learning_rate": 0.0002385245901639344,
      "loss": 1.8006,
      "step": 1650
    },
    {
      "epoch": 0.4331941544885177,
      "grad_norm": 1.120301604270935,
      "learning_rate": 0.00023812797461660496,
      "loss": 1.8579,
      "step": 1660
    },
    {
      "epoch": 0.43580375782881003,
      "grad_norm": 1.0295169353485107,
      "learning_rate": 0.00023773135906927548,
      "loss": 1.8342,
      "step": 1670
    },
    {
      "epoch": 0.4384133611691023,
      "grad_norm": 1.1983357667922974,
      "learning_rate": 0.000237334743521946,
      "loss": 1.8674,
      "step": 1680
    },
    {
      "epoch": 0.4410229645093946,
      "grad_norm": 1.2045501470565796,
      "learning_rate": 0.0002369381279746166,
      "loss": 1.8763,
      "step": 1690
    },
    {
      "epoch": 0.44363256784968685,
      "grad_norm": 0.897794783115387,
      "learning_rate": 0.00023654151242728712,
      "loss": 1.858,
      "step": 1700
    },
    {
      "epoch": 0.4462421711899791,
      "grad_norm": 0.9339864253997803,
      "learning_rate": 0.00023614489687995768,
      "loss": 1.739,
      "step": 1710
    },
    {
      "epoch": 0.4488517745302714,
      "grad_norm": 1.1480460166931152,
      "learning_rate": 0.0002357482813326282,
      "loss": 2.1209,
      "step": 1720
    },
    {
      "epoch": 0.4514613778705637,
      "grad_norm": 1.0375217199325562,
      "learning_rate": 0.00023535166578529876,
      "loss": 1.7881,
      "step": 1730
    },
    {
      "epoch": 0.45407098121085593,
      "grad_norm": 1.257396936416626,
      "learning_rate": 0.0002349550502379693,
      "loss": 1.9062,
      "step": 1740
    },
    {
      "epoch": 0.45668058455114824,
      "grad_norm": 1.1976999044418335,
      "learning_rate": 0.00023455843469063987,
      "loss": 1.7427,
      "step": 1750
    },
    {
      "epoch": 0.4592901878914405,
      "grad_norm": 1.1692672967910767,
      "learning_rate": 0.0002341618191433104,
      "loss": 1.9882,
      "step": 1760
    },
    {
      "epoch": 0.4618997912317328,
      "grad_norm": 0.823209822177887,
      "learning_rate": 0.00023376520359598096,
      "loss": 1.8643,
      "step": 1770
    },
    {
      "epoch": 0.46450939457202506,
      "grad_norm": 1.4776183366775513,
      "learning_rate": 0.00023336858804865149,
      "loss": 1.6935,
      "step": 1780
    },
    {
      "epoch": 0.4671189979123173,
      "grad_norm": 1.0522730350494385,
      "learning_rate": 0.00023297197250132204,
      "loss": 2.1569,
      "step": 1790
    },
    {
      "epoch": 0.4697286012526096,
      "grad_norm": 1.2398947477340698,
      "learning_rate": 0.00023257535695399257,
      "loss": 1.9595,
      "step": 1800
    },
    {
      "epoch": 0.4697286012526096,
      "eval_loss": 1.8440767526626587,
      "eval_runtime": 1.9534,
      "eval_samples_per_second": 61.43,
      "eval_steps_per_second": 7.679,
      "step": 1800
    },
    {
      "epoch": 0.4723382045929019,
      "grad_norm": 1.0517536401748657,
      "learning_rate": 0.0002321787414066631,
      "loss": 1.9219,
      "step": 1810
    },
    {
      "epoch": 0.47494780793319413,
      "grad_norm": 1.0960103273391724,
      "learning_rate": 0.00023178212585933365,
      "loss": 1.8157,
      "step": 1820
    },
    {
      "epoch": 0.47755741127348644,
      "grad_norm": 1.1070889234542847,
      "learning_rate": 0.0002313855103120042,
      "loss": 2.0303,
      "step": 1830
    },
    {
      "epoch": 0.4801670146137787,
      "grad_norm": 1.4665416479110718,
      "learning_rate": 0.00023098889476467477,
      "loss": 1.8667,
      "step": 1840
    },
    {
      "epoch": 0.482776617954071,
      "grad_norm": 1.023608922958374,
      "learning_rate": 0.0002305922792173453,
      "loss": 1.7141,
      "step": 1850
    },
    {
      "epoch": 0.48538622129436326,
      "grad_norm": 1.1631627082824707,
      "learning_rate": 0.00023019566367001585,
      "loss": 1.9068,
      "step": 1860
    },
    {
      "epoch": 0.4879958246346555,
      "grad_norm": 1.0675479173660278,
      "learning_rate": 0.00022979904812268638,
      "loss": 1.8188,
      "step": 1870
    },
    {
      "epoch": 0.4906054279749478,
      "grad_norm": 1.2747204303741455,
      "learning_rate": 0.00022940243257535693,
      "loss": 1.8301,
      "step": 1880
    },
    {
      "epoch": 0.4932150313152401,
      "grad_norm": 1.3450415134429932,
      "learning_rate": 0.00022900581702802746,
      "loss": 1.8265,
      "step": 1890
    },
    {
      "epoch": 0.49582463465553234,
      "grad_norm": 0.9141358733177185,
      "learning_rate": 0.00022860920148069804,
      "loss": 1.8479,
      "step": 1900
    },
    {
      "epoch": 0.49843423799582465,
      "grad_norm": 1.2548624277114868,
      "learning_rate": 0.00022821258593336857,
      "loss": 1.8501,
      "step": 1910
    },
    {
      "epoch": 0.5010438413361169,
      "grad_norm": 1.1910970211029053,
      "learning_rate": 0.00022781597038603913,
      "loss": 1.5536,
      "step": 1920
    },
    {
      "epoch": 0.5036534446764092,
      "grad_norm": 0.9629404544830322,
      "learning_rate": 0.00022741935483870966,
      "loss": 1.9002,
      "step": 1930
    },
    {
      "epoch": 0.5062630480167014,
      "grad_norm": 1.6994699239730835,
      "learning_rate": 0.0002270227392913802,
      "loss": 1.7629,
      "step": 1940
    },
    {
      "epoch": 0.5088726513569938,
      "grad_norm": 1.066749095916748,
      "learning_rate": 0.00022662612374405074,
      "loss": 1.8911,
      "step": 1950
    },
    {
      "epoch": 0.511482254697286,
      "grad_norm": 1.0237013101577759,
      "learning_rate": 0.00022622950819672127,
      "loss": 1.9854,
      "step": 1960
    },
    {
      "epoch": 0.5140918580375783,
      "grad_norm": 2.585606098175049,
      "learning_rate": 0.00022583289264939185,
      "loss": 1.9497,
      "step": 1970
    },
    {
      "epoch": 0.5167014613778705,
      "grad_norm": 1.159362554550171,
      "learning_rate": 0.00022543627710206238,
      "loss": 1.7762,
      "step": 1980
    },
    {
      "epoch": 0.5193110647181628,
      "grad_norm": 1.0727800130844116,
      "learning_rate": 0.00022503966155473294,
      "loss": 1.8343,
      "step": 1990
    },
    {
      "epoch": 0.5219206680584552,
      "grad_norm": 1.201766014099121,
      "learning_rate": 0.00022464304600740346,
      "loss": 1.8949,
      "step": 2000
    },
    {
      "epoch": 0.5219206680584552,
      "eval_loss": 1.821029782295227,
      "eval_runtime": 1.9692,
      "eval_samples_per_second": 60.938,
      "eval_steps_per_second": 7.617,
      "step": 2000
    },
    {
      "epoch": 0.5245302713987474,
      "grad_norm": 1.0538979768753052,
      "learning_rate": 0.00022424643046007402,
      "loss": 1.6671,
      "step": 2010
    },
    {
      "epoch": 0.5271398747390397,
      "grad_norm": 1.070385456085205,
      "learning_rate": 0.00022384981491274455,
      "loss": 1.8476,
      "step": 2020
    },
    {
      "epoch": 0.5297494780793319,
      "grad_norm": 1.3430516719818115,
      "learning_rate": 0.0002234531993654151,
      "loss": 1.74,
      "step": 2030
    },
    {
      "epoch": 0.5323590814196242,
      "grad_norm": 0.8833569288253784,
      "learning_rate": 0.00022305658381808563,
      "loss": 1.7587,
      "step": 2040
    },
    {
      "epoch": 0.5349686847599165,
      "grad_norm": 0.9619536995887756,
      "learning_rate": 0.00022265996827075621,
      "loss": 1.7978,
      "step": 2050
    },
    {
      "epoch": 0.5375782881002088,
      "grad_norm": 1.0299749374389648,
      "learning_rate": 0.00022226335272342674,
      "loss": 1.7553,
      "step": 2060
    },
    {
      "epoch": 0.5401878914405011,
      "grad_norm": 1.089105248451233,
      "learning_rate": 0.0002218667371760973,
      "loss": 1.8016,
      "step": 2070
    },
    {
      "epoch": 0.5427974947807933,
      "grad_norm": 1.655785083770752,
      "learning_rate": 0.00022147012162876783,
      "loss": 1.7115,
      "step": 2080
    },
    {
      "epoch": 0.5454070981210856,
      "grad_norm": 1.0081899166107178,
      "learning_rate": 0.00022107350608143838,
      "loss": 1.8631,
      "step": 2090
    },
    {
      "epoch": 0.5480167014613778,
      "grad_norm": 0.8879296779632568,
      "learning_rate": 0.0002206768905341089,
      "loss": 1.9072,
      "step": 2100
    },
    {
      "epoch": 0.5506263048016702,
      "grad_norm": 1.3531972169876099,
      "learning_rate": 0.00022028027498677944,
      "loss": 1.8233,
      "step": 2110
    },
    {
      "epoch": 0.5532359081419624,
      "grad_norm": 1.0113765001296997,
      "learning_rate": 0.00021988365943945002,
      "loss": 1.6383,
      "step": 2120
    },
    {
      "epoch": 0.5558455114822547,
      "grad_norm": 1.0243219137191772,
      "learning_rate": 0.00021948704389212055,
      "loss": 1.8414,
      "step": 2130
    },
    {
      "epoch": 0.558455114822547,
      "grad_norm": 1.0358562469482422,
      "learning_rate": 0.0002190904283447911,
      "loss": 1.7756,
      "step": 2140
    },
    {
      "epoch": 0.5610647181628392,
      "grad_norm": 0.9542903900146484,
      "learning_rate": 0.00021869381279746163,
      "loss": 1.5496,
      "step": 2150
    },
    {
      "epoch": 0.5636743215031316,
      "grad_norm": 1.344484806060791,
      "learning_rate": 0.0002182971972501322,
      "loss": 1.9197,
      "step": 2160
    },
    {
      "epoch": 0.5662839248434238,
      "grad_norm": 0.7977942228317261,
      "learning_rate": 0.00021790058170280272,
      "loss": 1.8531,
      "step": 2170
    },
    {
      "epoch": 0.5688935281837161,
      "grad_norm": 0.872673749923706,
      "learning_rate": 0.00021750396615547327,
      "loss": 1.6928,
      "step": 2180
    },
    {
      "epoch": 0.5715031315240083,
      "grad_norm": 1.6202131509780884,
      "learning_rate": 0.0002171073506081438,
      "loss": 1.6832,
      "step": 2190
    },
    {
      "epoch": 0.5741127348643006,
      "grad_norm": 1.2814327478408813,
      "learning_rate": 0.00021671073506081438,
      "loss": 1.7056,
      "step": 2200
    },
    {
      "epoch": 0.5741127348643006,
      "eval_loss": 1.8054629564285278,
      "eval_runtime": 1.954,
      "eval_samples_per_second": 61.412,
      "eval_steps_per_second": 7.676,
      "step": 2200
    },
    {
      "epoch": 0.576722338204593,
      "grad_norm": 1.3460625410079956,
      "learning_rate": 0.0002163141195134849,
      "loss": 1.8675,
      "step": 2210
    },
    {
      "epoch": 0.5793319415448852,
      "grad_norm": 0.9454607963562012,
      "learning_rate": 0.00021591750396615547,
      "loss": 1.8242,
      "step": 2220
    },
    {
      "epoch": 0.5819415448851775,
      "grad_norm": 1.0317028760910034,
      "learning_rate": 0.000215520888418826,
      "loss": 1.6802,
      "step": 2230
    },
    {
      "epoch": 0.5845511482254697,
      "grad_norm": 1.0284144878387451,
      "learning_rate": 0.00021512427287149653,
      "loss": 1.7461,
      "step": 2240
    },
    {
      "epoch": 0.587160751565762,
      "grad_norm": 1.1642612218856812,
      "learning_rate": 0.00021472765732416708,
      "loss": 1.6341,
      "step": 2250
    },
    {
      "epoch": 0.5897703549060542,
      "grad_norm": 1.0240139961242676,
      "learning_rate": 0.0002143310417768376,
      "loss": 2.0642,
      "step": 2260
    },
    {
      "epoch": 0.5923799582463466,
      "grad_norm": 1.026434302330017,
      "learning_rate": 0.0002139344262295082,
      "loss": 1.7017,
      "step": 2270
    },
    {
      "epoch": 0.5949895615866388,
      "grad_norm": 0.9628187417984009,
      "learning_rate": 0.00021353781068217872,
      "loss": 1.9035,
      "step": 2280
    },
    {
      "epoch": 0.5975991649269311,
      "grad_norm": 1.1340595483779907,
      "learning_rate": 0.00021314119513484928,
      "loss": 1.6561,
      "step": 2290
    },
    {
      "epoch": 0.6002087682672234,
      "grad_norm": 0.995076596736908,
      "learning_rate": 0.0002127445795875198,
      "loss": 1.953,
      "step": 2300
    },
    {
      "epoch": 0.6028183716075156,
      "grad_norm": 1.138375163078308,
      "learning_rate": 0.00021234796404019036,
      "loss": 1.8285,
      "step": 2310
    },
    {
      "epoch": 0.605427974947808,
      "grad_norm": 0.9906894564628601,
      "learning_rate": 0.0002119513484928609,
      "loss": 1.7356,
      "step": 2320
    },
    {
      "epoch": 0.6080375782881002,
      "grad_norm": 1.1203153133392334,
      "learning_rate": 0.00021155473294553147,
      "loss": 1.8879,
      "step": 2330
    },
    {
      "epoch": 0.6106471816283925,
      "grad_norm": 1.002901554107666,
      "learning_rate": 0.000211158117398202,
      "loss": 1.7331,
      "step": 2340
    },
    {
      "epoch": 0.6132567849686847,
      "grad_norm": 1.1610305309295654,
      "learning_rate": 0.00021076150185087256,
      "loss": 1.8908,
      "step": 2350
    },
    {
      "epoch": 0.615866388308977,
      "grad_norm": 2.1733779907226562,
      "learning_rate": 0.00021036488630354308,
      "loss": 2.1325,
      "step": 2360
    },
    {
      "epoch": 0.6184759916492694,
      "grad_norm": 1.4601985216140747,
      "learning_rate": 0.00020996827075621364,
      "loss": 1.8202,
      "step": 2370
    },
    {
      "epoch": 0.6210855949895616,
      "grad_norm": 1.080256462097168,
      "learning_rate": 0.0002096113167636171,
      "loss": 1.8642,
      "step": 2380
    },
    {
      "epoch": 0.6236951983298539,
      "grad_norm": 1.0851657390594482,
      "learning_rate": 0.00020921470121628764,
      "loss": 1.933,
      "step": 2390
    },
    {
      "epoch": 0.6263048016701461,
      "grad_norm": 1.2480601072311401,
      "learning_rate": 0.00020881808566895822,
      "loss": 1.9387,
      "step": 2400
    },
    {
      "epoch": 0.6263048016701461,
      "eval_loss": 1.785852313041687,
      "eval_runtime": 1.9571,
      "eval_samples_per_second": 61.315,
      "eval_steps_per_second": 7.664,
      "step": 2400
    },
    {
      "epoch": 0.6289144050104384,
      "grad_norm": 1.1524629592895508,
      "learning_rate": 0.00020842147012162875,
      "loss": 1.6444,
      "step": 2410
    },
    {
      "epoch": 0.6315240083507306,
      "grad_norm": 1.2291483879089355,
      "learning_rate": 0.0002080248545742993,
      "loss": 1.6422,
      "step": 2420
    },
    {
      "epoch": 0.634133611691023,
      "grad_norm": 1.3970850706100464,
      "learning_rate": 0.00020762823902696983,
      "loss": 1.7689,
      "step": 2430
    },
    {
      "epoch": 0.6367432150313153,
      "grad_norm": 1.1633710861206055,
      "learning_rate": 0.0002072316234796404,
      "loss": 1.8807,
      "step": 2440
    },
    {
      "epoch": 0.6393528183716075,
      "grad_norm": 1.0064160823822021,
      "learning_rate": 0.00020683500793231092,
      "loss": 1.9699,
      "step": 2450
    },
    {
      "epoch": 0.6419624217118998,
      "grad_norm": 1.4277434349060059,
      "learning_rate": 0.0002064383923849815,
      "loss": 1.9386,
      "step": 2460
    },
    {
      "epoch": 0.644572025052192,
      "grad_norm": 0.9182542562484741,
      "learning_rate": 0.00020604177683765203,
      "loss": 1.5653,
      "step": 2470
    },
    {
      "epoch": 0.6471816283924844,
      "grad_norm": 1.1335663795471191,
      "learning_rate": 0.00020564516129032256,
      "loss": 1.8816,
      "step": 2480
    },
    {
      "epoch": 0.6497912317327766,
      "grad_norm": 1.148482322692871,
      "learning_rate": 0.0002052485457429931,
      "loss": 1.7713,
      "step": 2490
    },
    {
      "epoch": 0.6524008350730689,
      "grad_norm": 1.0485641956329346,
      "learning_rate": 0.00020485193019566364,
      "loss": 1.7975,
      "step": 2500
    },
    {
      "epoch": 0.6550104384133612,
      "grad_norm": 0.9876821041107178,
      "learning_rate": 0.0002044553146483342,
      "loss": 1.9327,
      "step": 2510
    },
    {
      "epoch": 0.6576200417536534,
      "grad_norm": 1.4584425687789917,
      "learning_rate": 0.00020405869910100472,
      "loss": 1.7403,
      "step": 2520
    },
    {
      "epoch": 0.6602296450939458,
      "grad_norm": 1.0933736562728882,
      "learning_rate": 0.00020366208355367528,
      "loss": 1.7178,
      "step": 2530
    },
    {
      "epoch": 0.662839248434238,
      "grad_norm": 1.0775964260101318,
      "learning_rate": 0.00020326546800634584,
      "loss": 1.8119,
      "step": 2540
    },
    {
      "epoch": 0.6654488517745303,
      "grad_norm": 0.9619295001029968,
      "learning_rate": 0.0002028688524590164,
      "loss": 1.8367,
      "step": 2550
    },
    {
      "epoch": 0.6680584551148225,
      "grad_norm": 0.9690175652503967,
      "learning_rate": 0.00020247223691168692,
      "loss": 1.9375,
      "step": 2560
    },
    {
      "epoch": 0.6706680584551148,
      "grad_norm": 0.8218242526054382,
      "learning_rate": 0.00020207562136435747,
      "loss": 2.0092,
      "step": 2570
    },
    {
      "epoch": 0.673277661795407,
      "grad_norm": 0.9700257182121277,
      "learning_rate": 0.000201679005817028,
      "loss": 1.7918,
      "step": 2580
    },
    {
      "epoch": 0.6758872651356994,
      "grad_norm": 1.1704225540161133,
      "learning_rate": 0.00020128239026969856,
      "loss": 1.6299,
      "step": 2590
    },
    {
      "epoch": 0.6784968684759917,
      "grad_norm": 1.1900248527526855,
      "learning_rate": 0.0002008857747223691,
      "loss": 1.8171,
      "step": 2600
    },
    {
      "epoch": 0.6784968684759917,
      "eval_loss": 1.7682254314422607,
      "eval_runtime": 1.9558,
      "eval_samples_per_second": 61.355,
      "eval_steps_per_second": 7.669,
      "step": 2600
    },
    {
      "epoch": 0.6811064718162839,
      "grad_norm": 1.0570578575134277,
      "learning_rate": 0.00020048915917503967,
      "loss": 1.8322,
      "step": 2610
    },
    {
      "epoch": 0.6837160751565762,
      "grad_norm": 1.0071076154708862,
      "learning_rate": 0.0002000925436277102,
      "loss": 1.9192,
      "step": 2620
    },
    {
      "epoch": 0.6863256784968684,
      "grad_norm": 1.209484338760376,
      "learning_rate": 0.00019969592808038073,
      "loss": 1.6106,
      "step": 2630
    },
    {
      "epoch": 0.6889352818371608,
      "grad_norm": 1.1682791709899902,
      "learning_rate": 0.00019929931253305128,
      "loss": 1.6554,
      "step": 2640
    },
    {
      "epoch": 0.691544885177453,
      "grad_norm": 1.018115520477295,
      "learning_rate": 0.0001989026969857218,
      "loss": 1.8054,
      "step": 2650
    },
    {
      "epoch": 0.6941544885177453,
      "grad_norm": 1.0842746496200562,
      "learning_rate": 0.00019850608143839237,
      "loss": 1.8044,
      "step": 2660
    },
    {
      "epoch": 0.6967640918580376,
      "grad_norm": 1.1680330038070679,
      "learning_rate": 0.0001981094658910629,
      "loss": 1.7821,
      "step": 2670
    },
    {
      "epoch": 0.6993736951983298,
      "grad_norm": 1.181059718132019,
      "learning_rate": 0.00019771285034373348,
      "loss": 1.9473,
      "step": 2680
    },
    {
      "epoch": 0.7019832985386222,
      "grad_norm": 0.9635204672813416,
      "learning_rate": 0.000197316234796404,
      "loss": 1.7123,
      "step": 2690
    },
    {
      "epoch": 0.7045929018789144,
      "grad_norm": 0.9909036755561829,
      "learning_rate": 0.00019691961924907456,
      "loss": 1.601,
      "step": 2700
    },
    {
      "epoch": 0.7072025052192067,
      "grad_norm": 1.1142066717147827,
      "learning_rate": 0.0001965230037017451,
      "loss": 1.7454,
      "step": 2710
    },
    {
      "epoch": 0.7098121085594989,
      "grad_norm": 1.4479718208312988,
      "learning_rate": 0.00019612638815441564,
      "loss": 1.7723,
      "step": 2720
    },
    {
      "epoch": 0.7124217118997912,
      "grad_norm": 1.226146936416626,
      "learning_rate": 0.00019572977260708617,
      "loss": 1.7458,
      "step": 2730
    },
    {
      "epoch": 0.7150313152400835,
      "grad_norm": 1.0753246545791626,
      "learning_rate": 0.00019533315705975673,
      "loss": 1.8224,
      "step": 2740
    },
    {
      "epoch": 0.7176409185803758,
      "grad_norm": 1.3343629837036133,
      "learning_rate": 0.00019493654151242726,
      "loss": 1.9404,
      "step": 2750
    },
    {
      "epoch": 0.7202505219206681,
      "grad_norm": 1.46229088306427,
      "learning_rate": 0.00019453992596509779,
      "loss": 1.9678,
      "step": 2760
    },
    {
      "epoch": 0.7228601252609603,
      "grad_norm": 1.0469787120819092,
      "learning_rate": 0.00019414331041776837,
      "loss": 1.8785,
      "step": 2770
    },
    {
      "epoch": 0.7254697286012526,
      "grad_norm": 1.0090759992599487,
      "learning_rate": 0.0001937466948704389,
      "loss": 1.8009,
      "step": 2780
    },
    {
      "epoch": 0.7280793319415448,
      "grad_norm": 1.0604665279388428,
      "learning_rate": 0.00019335007932310945,
      "loss": 1.7865,
      "step": 2790
    },
    {
      "epoch": 0.7306889352818372,
      "grad_norm": 1.075354814529419,
      "learning_rate": 0.00019295346377577998,
      "loss": 1.8355,
      "step": 2800
    },
    {
      "epoch": 0.7306889352818372,
      "eval_loss": 1.7624698877334595,
      "eval_runtime": 1.9596,
      "eval_samples_per_second": 61.236,
      "eval_steps_per_second": 7.654,
      "step": 2800
    },
    {
      "epoch": 0.7332985386221295,
      "grad_norm": 1.2705086469650269,
      "learning_rate": 0.00019255684822845054,
      "loss": 1.5387,
      "step": 2810
    },
    {
      "epoch": 0.7359081419624217,
      "grad_norm": 1.5250266790390015,
      "learning_rate": 0.00019216023268112106,
      "loss": 1.6201,
      "step": 2820
    },
    {
      "epoch": 0.738517745302714,
      "grad_norm": 1.0521849393844604,
      "learning_rate": 0.00019176361713379165,
      "loss": 1.7155,
      "step": 2830
    },
    {
      "epoch": 0.7411273486430062,
      "grad_norm": 1.4210947751998901,
      "learning_rate": 0.00019136700158646218,
      "loss": 1.7756,
      "step": 2840
    },
    {
      "epoch": 0.7437369519832986,
      "grad_norm": 1.2643967866897583,
      "learning_rate": 0.00019097038603913273,
      "loss": 1.7209,
      "step": 2850
    },
    {
      "epoch": 0.7463465553235908,
      "grad_norm": 0.9959920048713684,
      "learning_rate": 0.00019057377049180326,
      "loss": 1.6799,
      "step": 2860
    },
    {
      "epoch": 0.7489561586638831,
      "grad_norm": 1.197263479232788,
      "learning_rate": 0.00019017715494447382,
      "loss": 1.8011,
      "step": 2870
    },
    {
      "epoch": 0.7515657620041754,
      "grad_norm": 1.3735913038253784,
      "learning_rate": 0.00018978053939714434,
      "loss": 1.9263,
      "step": 2880
    },
    {
      "epoch": 0.7541753653444676,
      "grad_norm": 1.4330227375030518,
      "learning_rate": 0.0001893839238498149,
      "loss": 1.8042,
      "step": 2890
    },
    {
      "epoch": 0.7567849686847599,
      "grad_norm": 1.0612865686416626,
      "learning_rate": 0.00018898730830248543,
      "loss": 1.7408,
      "step": 2900
    },
    {
      "epoch": 0.7593945720250522,
      "grad_norm": 1.3847053050994873,
      "learning_rate": 0.00018859069275515598,
      "loss": 1.6739,
      "step": 2910
    },
    {
      "epoch": 0.7620041753653445,
      "grad_norm": 1.148077130317688,
      "learning_rate": 0.00018819407720782654,
      "loss": 1.9582,
      "step": 2920
    },
    {
      "epoch": 0.7646137787056367,
      "grad_norm": 1.2495864629745483,
      "learning_rate": 0.00018779746166049707,
      "loss": 1.8276,
      "step": 2930
    },
    {
      "epoch": 0.767223382045929,
      "grad_norm": 1.0939778089523315,
      "learning_rate": 0.00018740084611316762,
      "loss": 1.8153,
      "step": 2940
    },
    {
      "epoch": 0.7698329853862212,
      "grad_norm": 1.4627372026443481,
      "learning_rate": 0.00018700423056583815,
      "loss": 1.9466,
      "step": 2950
    },
    {
      "epoch": 0.7724425887265136,
      "grad_norm": 0.9419782757759094,
      "learning_rate": 0.0001866076150185087,
      "loss": 1.9335,
      "step": 2960
    },
    {
      "epoch": 0.7750521920668059,
      "grad_norm": 1.679368257522583,
      "learning_rate": 0.00018621099947117923,
      "loss": 1.7372,
      "step": 2970
    },
    {
      "epoch": 0.7776617954070981,
      "grad_norm": 1.115796446800232,
      "learning_rate": 0.00018581438392384982,
      "loss": 1.6928,
      "step": 2980
    },
    {
      "epoch": 0.7802713987473904,
      "grad_norm": 1.043784737586975,
      "learning_rate": 0.00018541776837652035,
      "loss": 1.8352,
      "step": 2990
    },
    {
      "epoch": 0.7828810020876826,
      "grad_norm": 1.2368519306182861,
      "learning_rate": 0.0001850211528291909,
      "loss": 1.6238,
      "step": 3000
    },
    {
      "epoch": 0.7828810020876826,
      "eval_loss": 1.7489861249923706,
      "eval_runtime": 1.9554,
      "eval_samples_per_second": 61.369,
      "eval_steps_per_second": 7.671,
      "step": 3000
    },
    {
      "epoch": 0.785490605427975,
      "grad_norm": 1.120086669921875,
      "learning_rate": 0.00018462453728186143,
      "loss": 1.7518,
      "step": 3010
    },
    {
      "epoch": 0.7881002087682673,
      "grad_norm": 1.1559975147247314,
      "learning_rate": 0.00018422792173453199,
      "loss": 1.885,
      "step": 3020
    },
    {
      "epoch": 0.7907098121085595,
      "grad_norm": 1.0715692043304443,
      "learning_rate": 0.00018383130618720251,
      "loss": 1.7722,
      "step": 3030
    },
    {
      "epoch": 0.7933194154488518,
      "grad_norm": 1.2339215278625488,
      "learning_rate": 0.0001834346906398731,
      "loss": 1.7172,
      "step": 3040
    },
    {
      "epoch": 0.795929018789144,
      "grad_norm": 1.0184507369995117,
      "learning_rate": 0.00018303807509254362,
      "loss": 1.5443,
      "step": 3050
    },
    {
      "epoch": 0.7985386221294363,
      "grad_norm": 1.0686908960342407,
      "learning_rate": 0.00018264145954521415,
      "loss": 1.809,
      "step": 3060
    },
    {
      "epoch": 0.8011482254697286,
      "grad_norm": 0.9544336795806885,
      "learning_rate": 0.0001822448439978847,
      "loss": 1.7224,
      "step": 3070
    },
    {
      "epoch": 0.8037578288100209,
      "grad_norm": 1.260351300239563,
      "learning_rate": 0.00018184822845055524,
      "loss": 1.7196,
      "step": 3080
    },
    {
      "epoch": 0.8063674321503131,
      "grad_norm": 1.237060785293579,
      "learning_rate": 0.0001814516129032258,
      "loss": 1.9184,
      "step": 3090
    },
    {
      "epoch": 0.8089770354906054,
      "grad_norm": 1.116373896598816,
      "learning_rate": 0.00018105499735589632,
      "loss": 1.7733,
      "step": 3100
    },
    {
      "epoch": 0.8115866388308977,
      "grad_norm": 1.4817168712615967,
      "learning_rate": 0.00018065838180856688,
      "loss": 1.8037,
      "step": 3110
    },
    {
      "epoch": 0.81419624217119,
      "grad_norm": 1.197554588317871,
      "learning_rate": 0.0001802617662612374,
      "loss": 1.9243,
      "step": 3120
    },
    {
      "epoch": 0.8168058455114823,
      "grad_norm": 1.3639599084854126,
      "learning_rate": 0.000179865150713908,
      "loss": 1.8875,
      "step": 3130
    },
    {
      "epoch": 0.8194154488517745,
      "grad_norm": 1.2372599840164185,
      "learning_rate": 0.00017946853516657852,
      "loss": 1.6464,
      "step": 3140
    },
    {
      "epoch": 0.8220250521920668,
      "grad_norm": 1.1090693473815918,
      "learning_rate": 0.00017907191961924907,
      "loss": 1.7021,
      "step": 3150
    },
    {
      "epoch": 0.824634655532359,
      "grad_norm": 1.046858787536621,
      "learning_rate": 0.0001786753040719196,
      "loss": 1.6665,
      "step": 3160
    },
    {
      "epoch": 0.8272442588726514,
      "grad_norm": 1.1705139875411987,
      "learning_rate": 0.00017827868852459016,
      "loss": 1.6079,
      "step": 3170
    },
    {
      "epoch": 0.8298538622129437,
      "grad_norm": 1.0796295404434204,
      "learning_rate": 0.00017788207297726068,
      "loss": 1.7761,
      "step": 3180
    },
    {
      "epoch": 0.8324634655532359,
      "grad_norm": 1.4412364959716797,
      "learning_rate": 0.0001774854574299312,
      "loss": 1.6754,
      "step": 3190
    },
    {
      "epoch": 0.8350730688935282,
      "grad_norm": 1.1129732131958008,
      "learning_rate": 0.0001770888418826018,
      "loss": 1.7986,
      "step": 3200
    },
    {
      "epoch": 0.8350730688935282,
      "eval_loss": 1.738922357559204,
      "eval_runtime": 1.9605,
      "eval_samples_per_second": 61.21,
      "eval_steps_per_second": 7.651,
      "step": 3200
    },
    {
      "epoch": 0.8376826722338204,
      "grad_norm": 1.0543274879455566,
      "learning_rate": 0.00017669222633527232,
      "loss": 1.7074,
      "step": 3210
    },
    {
      "epoch": 0.8402922755741128,
      "grad_norm": 1.0721806287765503,
      "learning_rate": 0.00017629561078794288,
      "loss": 1.748,
      "step": 3220
    },
    {
      "epoch": 0.842901878914405,
      "grad_norm": 1.0198990106582642,
      "learning_rate": 0.0001758989952406134,
      "loss": 1.7303,
      "step": 3230
    },
    {
      "epoch": 0.8455114822546973,
      "grad_norm": 1.1679037809371948,
      "learning_rate": 0.00017550237969328396,
      "loss": 1.812,
      "step": 3240
    },
    {
      "epoch": 0.8481210855949896,
      "grad_norm": 1.178370714187622,
      "learning_rate": 0.0001751057641459545,
      "loss": 1.78,
      "step": 3250
    },
    {
      "epoch": 0.8507306889352818,
      "grad_norm": 0.9680695533752441,
      "learning_rate": 0.00017470914859862505,
      "loss": 1.8957,
      "step": 3260
    },
    {
      "epoch": 0.8533402922755741,
      "grad_norm": 1.0028092861175537,
      "learning_rate": 0.00017431253305129558,
      "loss": 1.8841,
      "step": 3270
    },
    {
      "epoch": 0.8559498956158664,
      "grad_norm": 1.0055383443832397,
      "learning_rate": 0.00017391591750396616,
      "loss": 1.869,
      "step": 3280
    },
    {
      "epoch": 0.8585594989561587,
      "grad_norm": 1.3251715898513794,
      "learning_rate": 0.0001735193019566367,
      "loss": 1.7833,
      "step": 3290
    },
    {
      "epoch": 0.8611691022964509,
      "grad_norm": 1.2061175107955933,
      "learning_rate": 0.00017312268640930724,
      "loss": 1.8722,
      "step": 3300
    },
    {
      "epoch": 0.8637787056367432,
      "grad_norm": 0.8376083374023438,
      "learning_rate": 0.00017272607086197777,
      "loss": 1.5554,
      "step": 3310
    },
    {
      "epoch": 0.8663883089770354,
      "grad_norm": 1.2268388271331787,
      "learning_rate": 0.00017232945531464833,
      "loss": 1.6004,
      "step": 3320
    },
    {
      "epoch": 0.8689979123173278,
      "grad_norm": 1.3833214044570923,
      "learning_rate": 0.00017193283976731885,
      "loss": 1.704,
      "step": 3330
    },
    {
      "epoch": 0.8716075156576201,
      "grad_norm": 0.9443008899688721,
      "learning_rate": 0.00017153622421998938,
      "loss": 1.7419,
      "step": 3340
    },
    {
      "epoch": 0.8742171189979123,
      "grad_norm": 1.2449792623519897,
      "learning_rate": 0.00017113960867265997,
      "loss": 1.9493,
      "step": 3350
    },
    {
      "epoch": 0.8768267223382046,
      "grad_norm": 1.1790777444839478,
      "learning_rate": 0.0001707429931253305,
      "loss": 1.6307,
      "step": 3360
    },
    {
      "epoch": 0.8794363256784968,
      "grad_norm": 0.8646610379219055,
      "learning_rate": 0.00017034637757800105,
      "loss": 1.6225,
      "step": 3370
    },
    {
      "epoch": 0.8820459290187892,
      "grad_norm": 0.9951653480529785,
      "learning_rate": 0.00016994976203067158,
      "loss": 1.7588,
      "step": 3380
    },
    {
      "epoch": 0.8846555323590815,
      "grad_norm": 1.6124881505966187,
      "learning_rate": 0.00016955314648334213,
      "loss": 1.8135,
      "step": 3390
    },
    {
      "epoch": 0.8872651356993737,
      "grad_norm": 1.4088486433029175,
      "learning_rate": 0.00016915653093601266,
      "loss": 1.8781,
      "step": 3400
    },
    {
      "epoch": 0.8872651356993737,
      "eval_loss": 1.7393871545791626,
      "eval_runtime": 1.9627,
      "eval_samples_per_second": 61.14,
      "eval_steps_per_second": 7.643,
      "step": 3400
    },
    {
      "epoch": 0.889874739039666,
      "grad_norm": 1.5343695878982544,
      "learning_rate": 0.00016875991538868324,
      "loss": 1.6034,
      "step": 3410
    },
    {
      "epoch": 0.8924843423799582,
      "grad_norm": 1.3630260229110718,
      "learning_rate": 0.00016836329984135377,
      "loss": 1.7456,
      "step": 3420
    },
    {
      "epoch": 0.8950939457202505,
      "grad_norm": 1.1712247133255005,
      "learning_rate": 0.00016796668429402433,
      "loss": 1.7344,
      "step": 3430
    },
    {
      "epoch": 0.8977035490605428,
      "grad_norm": 1.232142448425293,
      "learning_rate": 0.00016757006874669486,
      "loss": 1.878,
      "step": 3440
    },
    {
      "epoch": 0.9003131524008351,
      "grad_norm": 1.0380910634994507,
      "learning_rate": 0.0001671734531993654,
      "loss": 1.9092,
      "step": 3450
    },
    {
      "epoch": 0.9029227557411273,
      "grad_norm": 1.243471384048462,
      "learning_rate": 0.00016677683765203594,
      "loss": 1.8091,
      "step": 3460
    },
    {
      "epoch": 0.9055323590814196,
      "grad_norm": 1.0785269737243652,
      "learning_rate": 0.0001663802221047065,
      "loss": 1.5306,
      "step": 3470
    },
    {
      "epoch": 0.9081419624217119,
      "grad_norm": 1.2248536348342896,
      "learning_rate": 0.00016598360655737702,
      "loss": 1.8469,
      "step": 3480
    },
    {
      "epoch": 0.9107515657620042,
      "grad_norm": 1.5159035921096802,
      "learning_rate": 0.00016558699101004755,
      "loss": 1.739,
      "step": 3490
    },
    {
      "epoch": 0.9133611691022965,
      "grad_norm": 0.9832502007484436,
      "learning_rate": 0.00016519037546271814,
      "loss": 1.8275,
      "step": 3500
    },
    {
      "epoch": 0.9159707724425887,
      "grad_norm": 1.3594386577606201,
      "learning_rate": 0.00016479375991538866,
      "loss": 1.7785,
      "step": 3510
    },
    {
      "epoch": 0.918580375782881,
      "grad_norm": 1.3189970254898071,
      "learning_rate": 0.00016439714436805922,
      "loss": 1.6662,
      "step": 3520
    },
    {
      "epoch": 0.9211899791231732,
      "grad_norm": 1.0135520696640015,
      "learning_rate": 0.00016400052882072975,
      "loss": 1.6086,
      "step": 3530
    },
    {
      "epoch": 0.9237995824634656,
      "grad_norm": 1.3360979557037354,
      "learning_rate": 0.0001636039132734003,
      "loss": 1.8046,
      "step": 3540
    },
    {
      "epoch": 0.9264091858037579,
      "grad_norm": 1.0870683193206787,
      "learning_rate": 0.00016320729772607083,
      "loss": 1.8833,
      "step": 3550
    },
    {
      "epoch": 0.9290187891440501,
      "grad_norm": 1.0653702020645142,
      "learning_rate": 0.00016281068217874141,
      "loss": 1.9101,
      "step": 3560
    },
    {
      "epoch": 0.9316283924843424,
      "grad_norm": 1.2982237339019775,
      "learning_rate": 0.00016241406663141194,
      "loss": 1.7645,
      "step": 3570
    },
    {
      "epoch": 0.9342379958246346,
      "grad_norm": 1.6312943696975708,
      "learning_rate": 0.0001620174510840825,
      "loss": 1.9551,
      "step": 3580
    },
    {
      "epoch": 0.9368475991649269,
      "grad_norm": 1.2443214654922485,
      "learning_rate": 0.00016162083553675303,
      "loss": 1.7792,
      "step": 3590
    },
    {
      "epoch": 0.9394572025052192,
      "grad_norm": 1.0149790048599243,
      "learning_rate": 0.00016122421998942358,
      "loss": 1.7376,
      "step": 3600
    },
    {
      "epoch": 0.9394572025052192,
      "eval_loss": 1.7282263040542603,
      "eval_runtime": 1.9617,
      "eval_samples_per_second": 61.173,
      "eval_steps_per_second": 7.647,
      "step": 3600
    },
    {
      "epoch": 0.9420668058455115,
      "grad_norm": 1.3966871500015259,
      "learning_rate": 0.0001608276044420941,
      "loss": 1.7334,
      "step": 3610
    },
    {
      "epoch": 0.9446764091858038,
      "grad_norm": 1.1571253538131714,
      "learning_rate": 0.00016043098889476464,
      "loss": 1.7386,
      "step": 3620
    },
    {
      "epoch": 0.947286012526096,
      "grad_norm": 1.2747677564620972,
      "learning_rate": 0.0001600343733474352,
      "loss": 1.9273,
      "step": 3630
    },
    {
      "epoch": 0.9498956158663883,
      "grad_norm": 1.3625940084457397,
      "learning_rate": 0.00015963775780010572,
      "loss": 1.5883,
      "step": 3640
    },
    {
      "epoch": 0.9525052192066806,
      "grad_norm": 0.9731058478355408,
      "learning_rate": 0.0001592411422527763,
      "loss": 1.3173,
      "step": 3650
    },
    {
      "epoch": 0.9551148225469729,
      "grad_norm": 1.1180262565612793,
      "learning_rate": 0.00015884452670544683,
      "loss": 1.7109,
      "step": 3660
    },
    {
      "epoch": 0.9577244258872651,
      "grad_norm": 1.2344300746917725,
      "learning_rate": 0.0001584479111581174,
      "loss": 1.7355,
      "step": 3670
    },
    {
      "epoch": 0.9603340292275574,
      "grad_norm": 1.235690951347351,
      "learning_rate": 0.00015805129561078792,
      "loss": 1.6519,
      "step": 3680
    },
    {
      "epoch": 0.9629436325678496,
      "grad_norm": 1.352425456047058,
      "learning_rate": 0.00015765468006345847,
      "loss": 1.6488,
      "step": 3690
    },
    {
      "epoch": 0.965553235908142,
      "grad_norm": 1.239089012145996,
      "learning_rate": 0.000157258064516129,
      "loss": 1.7127,
      "step": 3700
    },
    {
      "epoch": 0.9681628392484343,
      "grad_norm": 1.476393699645996,
      "learning_rate": 0.00015686144896879958,
      "loss": 1.6985,
      "step": 3710
    },
    {
      "epoch": 0.9707724425887265,
      "grad_norm": 1.347849726676941,
      "learning_rate": 0.0001564648334214701,
      "loss": 1.6564,
      "step": 3720
    },
    {
      "epoch": 0.9733820459290188,
      "grad_norm": 0.9756587147712708,
      "learning_rate": 0.00015606821787414067,
      "loss": 1.643,
      "step": 3730
    },
    {
      "epoch": 0.975991649269311,
      "grad_norm": 1.27103853225708,
      "learning_rate": 0.0001556716023268112,
      "loss": 1.8552,
      "step": 3740
    },
    {
      "epoch": 0.9786012526096033,
      "grad_norm": 1.0400527715682983,
      "learning_rate": 0.00015527498677948175,
      "loss": 1.729,
      "step": 3750
    },
    {
      "epoch": 0.9812108559498957,
      "grad_norm": 1.1222628355026245,
      "learning_rate": 0.00015487837123215228,
      "loss": 1.4637,
      "step": 3760
    },
    {
      "epoch": 0.9838204592901879,
      "grad_norm": 0.9597196578979492,
      "learning_rate": 0.0001544817556848228,
      "loss": 1.6425,
      "step": 3770
    },
    {
      "epoch": 0.9864300626304802,
      "grad_norm": 1.0608415603637695,
      "learning_rate": 0.0001540851401374934,
      "loss": 1.5931,
      "step": 3780
    },
    {
      "epoch": 0.9890396659707724,
      "grad_norm": 1.918555736541748,
      "learning_rate": 0.00015368852459016392,
      "loss": 1.6299,
      "step": 3790
    },
    {
      "epoch": 0.9916492693110647,
      "grad_norm": 1.4336214065551758,
      "learning_rate": 0.00015329190904283448,
      "loss": 1.7398,
      "step": 3800
    },
    {
      "epoch": 0.9916492693110647,
      "eval_loss": 1.7206472158432007,
      "eval_runtime": 1.9604,
      "eval_samples_per_second": 61.213,
      "eval_steps_per_second": 7.652,
      "step": 3800
    },
    {
      "epoch": 0.994258872651357,
      "grad_norm": 0.9147927165031433,
      "learning_rate": 0.000152895293495505,
      "loss": 1.8815,
      "step": 3810
    },
    {
      "epoch": 0.9968684759916493,
      "grad_norm": 1.1923028230667114,
      "learning_rate": 0.00015249867794817556,
      "loss": 1.9121,
      "step": 3820
    },
    {
      "epoch": 0.9994780793319415,
      "grad_norm": 1.126860499382019,
      "learning_rate": 0.0001521020624008461,
      "loss": 1.7218,
      "step": 3830
    },
    {
      "epoch": 1.0020876826722338,
      "grad_norm": 1.1568655967712402,
      "learning_rate": 0.00015170544685351664,
      "loss": 1.7262,
      "step": 3840
    },
    {
      "epoch": 1.004697286012526,
      "grad_norm": 1.2931766510009766,
      "learning_rate": 0.00015130883130618717,
      "loss": 1.723,
      "step": 3850
    },
    {
      "epoch": 1.0073068893528183,
      "grad_norm": 1.2037113904953003,
      "learning_rate": 0.00015091221575885776,
      "loss": 1.6516,
      "step": 3860
    },
    {
      "epoch": 1.0099164926931106,
      "grad_norm": 0.873659074306488,
      "learning_rate": 0.00015051560021152828,
      "loss": 1.5641,
      "step": 3870
    },
    {
      "epoch": 1.0125260960334028,
      "grad_norm": 1.4451887607574463,
      "learning_rate": 0.00015011898466419884,
      "loss": 1.5952,
      "step": 3880
    },
    {
      "epoch": 1.0151356993736953,
      "grad_norm": 0.9451605677604675,
      "learning_rate": 0.00014972236911686937,
      "loss": 1.5192,
      "step": 3890
    },
    {
      "epoch": 1.0177453027139876,
      "grad_norm": 1.4486888647079468,
      "learning_rate": 0.00014932575356953992,
      "loss": 1.6847,
      "step": 3900
    },
    {
      "epoch": 1.0203549060542798,
      "grad_norm": 0.9956874847412109,
      "learning_rate": 0.00014892913802221045,
      "loss": 1.6868,
      "step": 3910
    },
    {
      "epoch": 1.022964509394572,
      "grad_norm": 1.3541240692138672,
      "learning_rate": 0.000148532522474881,
      "loss": 1.6509,
      "step": 3920
    },
    {
      "epoch": 1.0255741127348643,
      "grad_norm": 1.2135173082351685,
      "learning_rate": 0.00014813590692755156,
      "loss": 1.6496,
      "step": 3930
    },
    {
      "epoch": 1.0281837160751566,
      "grad_norm": 0.9986938238143921,
      "learning_rate": 0.0001477392913802221,
      "loss": 1.836,
      "step": 3940
    },
    {
      "epoch": 1.0307933194154488,
      "grad_norm": 1.440942645072937,
      "learning_rate": 0.00014734267583289265,
      "loss": 1.3643,
      "step": 3950
    },
    {
      "epoch": 1.033402922755741,
      "grad_norm": 1.2790048122406006,
      "learning_rate": 0.0001469460602855632,
      "loss": 1.5649,
      "step": 3960
    },
    {
      "epoch": 1.0360125260960333,
      "grad_norm": 1.0530829429626465,
      "learning_rate": 0.00014654944473823373,
      "loss": 1.7312,
      "step": 3970
    },
    {
      "epoch": 1.0386221294363256,
      "grad_norm": 1.3283324241638184,
      "learning_rate": 0.00014615282919090426,
      "loss": 1.8229,
      "step": 3980
    },
    {
      "epoch": 1.0412317327766178,
      "grad_norm": 1.5198405981063843,
      "learning_rate": 0.00014575621364357481,
      "loss": 1.6709,
      "step": 3990
    },
    {
      "epoch": 1.0438413361169103,
      "grad_norm": 1.2846463918685913,
      "learning_rate": 0.00014535959809624534,
      "loss": 1.595,
      "step": 4000
    },
    {
      "epoch": 1.0438413361169103,
      "eval_loss": 1.7165336608886719,
      "eval_runtime": 1.9545,
      "eval_samples_per_second": 61.397,
      "eval_steps_per_second": 7.675,
      "step": 4000
    },
    {
      "epoch": 1.0464509394572026,
      "grad_norm": 1.0627294778823853,
      "learning_rate": 0.0001449629825489159,
      "loss": 1.4444,
      "step": 4010
    },
    {
      "epoch": 1.0490605427974948,
      "grad_norm": 1.0308048725128174,
      "learning_rate": 0.00014456636700158645,
      "loss": 1.6188,
      "step": 4020
    },
    {
      "epoch": 1.051670146137787,
      "grad_norm": 1.4605164527893066,
      "learning_rate": 0.00014416975145425698,
      "loss": 1.4377,
      "step": 4030
    },
    {
      "epoch": 1.0542797494780793,
      "grad_norm": 1.4002127647399902,
      "learning_rate": 0.00014377313590692754,
      "loss": 1.4285,
      "step": 4040
    },
    {
      "epoch": 1.0568893528183716,
      "grad_norm": 1.1478908061981201,
      "learning_rate": 0.0001433765203595981,
      "loss": 1.7473,
      "step": 4050
    },
    {
      "epoch": 1.0594989561586639,
      "grad_norm": 1.063971996307373,
      "learning_rate": 0.00014297990481226862,
      "loss": 1.8436,
      "step": 4060
    },
    {
      "epoch": 1.062108559498956,
      "grad_norm": 1.1866390705108643,
      "learning_rate": 0.00014258328926493918,
      "loss": 1.5253,
      "step": 4070
    },
    {
      "epoch": 1.0647181628392484,
      "grad_norm": 1.228021264076233,
      "learning_rate": 0.00014218667371760973,
      "loss": 1.8046,
      "step": 4080
    },
    {
      "epoch": 1.0673277661795406,
      "grad_norm": 1.081900954246521,
      "learning_rate": 0.00014179005817028026,
      "loss": 1.6517,
      "step": 4090
    },
    {
      "epoch": 1.0699373695198329,
      "grad_norm": 2.5509071350097656,
      "learning_rate": 0.00014139344262295082,
      "loss": 1.6621,
      "step": 4100
    },
    {
      "epoch": 1.0725469728601253,
      "grad_norm": 1.1504899263381958,
      "learning_rate": 0.00014099682707562137,
      "loss": 1.5284,
      "step": 4110
    },
    {
      "epoch": 1.0751565762004176,
      "grad_norm": 1.186139702796936,
      "learning_rate": 0.0001406002115282919,
      "loss": 1.7048,
      "step": 4120
    },
    {
      "epoch": 1.0777661795407099,
      "grad_norm": 1.2780579328536987,
      "learning_rate": 0.00014020359598096243,
      "loss": 1.5491,
      "step": 4130
    },
    {
      "epoch": 1.0803757828810021,
      "grad_norm": 1.380940556526184,
      "learning_rate": 0.00013980698043363298,
      "loss": 1.5776,
      "step": 4140
    },
    {
      "epoch": 1.0829853862212944,
      "grad_norm": 1.0665738582611084,
      "learning_rate": 0.00013941036488630354,
      "loss": 1.7061,
      "step": 4150
    },
    {
      "epoch": 1.0855949895615866,
      "grad_norm": 0.9365150928497314,
      "learning_rate": 0.00013901374933897407,
      "loss": 1.655,
      "step": 4160
    },
    {
      "epoch": 1.0882045929018789,
      "grad_norm": 1.2349021434783936,
      "learning_rate": 0.00013861713379164462,
      "loss": 1.5977,
      "step": 4170
    },
    {
      "epoch": 1.0908141962421711,
      "grad_norm": 1.3211427927017212,
      "learning_rate": 0.00013822051824431515,
      "loss": 1.5905,
      "step": 4180
    },
    {
      "epoch": 1.0934237995824634,
      "grad_norm": 1.4641743898391724,
      "learning_rate": 0.0001378239026969857,
      "loss": 1.6186,
      "step": 4190
    },
    {
      "epoch": 1.0960334029227556,
      "grad_norm": 1.0737886428833008,
      "learning_rate": 0.00013742728714965626,
      "loss": 1.6354,
      "step": 4200
    },
    {
      "epoch": 1.0960334029227556,
      "eval_loss": 1.7044378519058228,
      "eval_runtime": 1.9618,
      "eval_samples_per_second": 61.167,
      "eval_steps_per_second": 7.646,
      "step": 4200
    },
    {
      "epoch": 1.0986430062630481,
      "grad_norm": 1.1287111043930054,
      "learning_rate": 0.0001370306716023268,
      "loss": 1.6861,
      "step": 4210
    },
    {
      "epoch": 1.1012526096033404,
      "grad_norm": 1.3614226579666138,
      "learning_rate": 0.00013663405605499735,
      "loss": 1.6971,
      "step": 4220
    },
    {
      "epoch": 1.1038622129436326,
      "grad_norm": 1.824530839920044,
      "learning_rate": 0.0001362374405076679,
      "loss": 1.596,
      "step": 4230
    },
    {
      "epoch": 1.1064718162839249,
      "grad_norm": 1.41425359249115,
      "learning_rate": 0.00013584082496033843,
      "loss": 1.5023,
      "step": 4240
    },
    {
      "epoch": 1.1090814196242171,
      "grad_norm": 1.364086627960205,
      "learning_rate": 0.000135444209413009,
      "loss": 1.8181,
      "step": 4250
    },
    {
      "epoch": 1.1116910229645094,
      "grad_norm": 1.4987332820892334,
      "learning_rate": 0.00013504759386567952,
      "loss": 1.847,
      "step": 4260
    },
    {
      "epoch": 1.1143006263048016,
      "grad_norm": 1.8794301748275757,
      "learning_rate": 0.00013465097831835007,
      "loss": 1.5618,
      "step": 4270
    },
    {
      "epoch": 1.116910229645094,
      "grad_norm": 1.2715857028961182,
      "learning_rate": 0.0001342543627710206,
      "loss": 1.7151,
      "step": 4280
    },
    {
      "epoch": 1.1195198329853862,
      "grad_norm": 1.3680938482284546,
      "learning_rate": 0.00013385774722369116,
      "loss": 1.6562,
      "step": 4290
    },
    {
      "epoch": 1.1221294363256784,
      "grad_norm": 1.3913042545318604,
      "learning_rate": 0.0001334611316763617,
      "loss": 1.8258,
      "step": 4300
    },
    {
      "epoch": 1.1247390396659709,
      "grad_norm": 1.3256587982177734,
      "learning_rate": 0.00013306451612903224,
      "loss": 1.6405,
      "step": 4310
    },
    {
      "epoch": 1.1273486430062631,
      "grad_norm": 1.6259697675704956,
      "learning_rate": 0.0001326679005817028,
      "loss": 1.8253,
      "step": 4320
    },
    {
      "epoch": 1.1299582463465554,
      "grad_norm": 1.297492504119873,
      "learning_rate": 0.00013227128503437335,
      "loss": 1.6115,
      "step": 4330
    },
    {
      "epoch": 1.1325678496868476,
      "grad_norm": 1.2129113674163818,
      "learning_rate": 0.00013187466948704388,
      "loss": 1.5177,
      "step": 4340
    },
    {
      "epoch": 1.13517745302714,
      "grad_norm": 1.2854288816452026,
      "learning_rate": 0.00013147805393971443,
      "loss": 1.6036,
      "step": 4350
    },
    {
      "epoch": 1.1377870563674322,
      "grad_norm": 1.2650517225265503,
      "learning_rate": 0.00013108143839238496,
      "loss": 1.3998,
      "step": 4360
    },
    {
      "epoch": 1.1403966597077244,
      "grad_norm": 1.3769677877426147,
      "learning_rate": 0.00013068482284505552,
      "loss": 1.7806,
      "step": 4370
    },
    {
      "epoch": 1.1430062630480167,
      "grad_norm": 1.1827036142349243,
      "learning_rate": 0.00013028820729772607,
      "loss": 1.8483,
      "step": 4380
    },
    {
      "epoch": 1.145615866388309,
      "grad_norm": 1.393280029296875,
      "learning_rate": 0.0001298915917503966,
      "loss": 1.4839,
      "step": 4390
    },
    {
      "epoch": 1.1482254697286012,
      "grad_norm": 1.5934251546859741,
      "learning_rate": 0.00012949497620306713,
      "loss": 1.4881,
      "step": 4400
    },
    {
      "epoch": 1.1482254697286012,
      "eval_loss": 1.7010161876678467,
      "eval_runtime": 1.9536,
      "eval_samples_per_second": 61.426,
      "eval_steps_per_second": 7.678,
      "step": 4400
    },
    {
      "epoch": 1.1508350730688934,
      "grad_norm": 1.32813560962677,
      "learning_rate": 0.00012909836065573769,
      "loss": 1.6859,
      "step": 4410
    },
    {
      "epoch": 1.153444676409186,
      "grad_norm": 1.6095341444015503,
      "learning_rate": 0.00012870174510840824,
      "loss": 1.7957,
      "step": 4420
    },
    {
      "epoch": 1.1560542797494782,
      "grad_norm": 1.243014931678772,
      "learning_rate": 0.00012830512956107877,
      "loss": 1.6272,
      "step": 4430
    },
    {
      "epoch": 1.1586638830897704,
      "grad_norm": 1.0468603372573853,
      "learning_rate": 0.00012790851401374933,
      "loss": 1.64,
      "step": 4440
    },
    {
      "epoch": 1.1612734864300627,
      "grad_norm": 1.433892846107483,
      "learning_rate": 0.00012751189846641988,
      "loss": 1.4927,
      "step": 4450
    },
    {
      "epoch": 1.163883089770355,
      "grad_norm": 1.2101589441299438,
      "learning_rate": 0.0001271152829190904,
      "loss": 1.5295,
      "step": 4460
    },
    {
      "epoch": 1.1664926931106472,
      "grad_norm": 1.257053017616272,
      "learning_rate": 0.00012671866737176096,
      "loss": 1.5892,
      "step": 4470
    },
    {
      "epoch": 1.1691022964509394,
      "grad_norm": 1.1247581243515015,
      "learning_rate": 0.00012632205182443152,
      "loss": 1.3392,
      "step": 4480
    },
    {
      "epoch": 1.1717118997912317,
      "grad_norm": 1.5344486236572266,
      "learning_rate": 0.00012592543627710205,
      "loss": 1.6612,
      "step": 4490
    },
    {
      "epoch": 1.174321503131524,
      "grad_norm": 1.4092681407928467,
      "learning_rate": 0.0001255288207297726,
      "loss": 1.4611,
      "step": 4500
    },
    {
      "epoch": 1.1769311064718162,
      "grad_norm": 1.2440415620803833,
      "learning_rate": 0.00012513220518244313,
      "loss": 1.5105,
      "step": 4510
    },
    {
      "epoch": 1.1795407098121085,
      "grad_norm": 1.6866248846054077,
      "learning_rate": 0.0001247355896351137,
      "loss": 1.5919,
      "step": 4520
    },
    {
      "epoch": 1.182150313152401,
      "grad_norm": 1.2810155153274536,
      "learning_rate": 0.00012433897408778424,
      "loss": 1.7097,
      "step": 4530
    },
    {
      "epoch": 1.1847599164926932,
      "grad_norm": 1.2756284475326538,
      "learning_rate": 0.00012394235854045477,
      "loss": 1.7034,
      "step": 4540
    },
    {
      "epoch": 1.1873695198329854,
      "grad_norm": 1.1381912231445312,
      "learning_rate": 0.0001235457429931253,
      "loss": 1.49,
      "step": 4550
    },
    {
      "epoch": 1.1899791231732777,
      "grad_norm": 1.8625423908233643,
      "learning_rate": 0.00012314912744579586,
      "loss": 1.5375,
      "step": 4560
    },
    {
      "epoch": 1.19258872651357,
      "grad_norm": 1.2738232612609863,
      "learning_rate": 0.0001227525118984664,
      "loss": 1.5273,
      "step": 4570
    },
    {
      "epoch": 1.1951983298538622,
      "grad_norm": 1.3928037881851196,
      "learning_rate": 0.00012235589635113694,
      "loss": 1.5442,
      "step": 4580
    },
    {
      "epoch": 1.1978079331941545,
      "grad_norm": 1.2503868341445923,
      "learning_rate": 0.0001219592808038075,
      "loss": 1.6494,
      "step": 4590
    },
    {
      "epoch": 1.2004175365344467,
      "grad_norm": 1.4276502132415771,
      "learning_rate": 0.00012156266525647804,
      "loss": 1.7461,
      "step": 4600
    },
    {
      "epoch": 1.2004175365344467,
      "eval_loss": 1.6910202503204346,
      "eval_runtime": 1.9599,
      "eval_samples_per_second": 61.229,
      "eval_steps_per_second": 7.654,
      "step": 4600
    },
    {
      "epoch": 1.203027139874739,
      "grad_norm": 1.5346406698226929,
      "learning_rate": 0.00012116604970914859,
      "loss": 1.4334,
      "step": 4610
    },
    {
      "epoch": 1.2056367432150312,
      "grad_norm": 1.3283647298812866,
      "learning_rate": 0.00012076943416181914,
      "loss": 1.6872,
      "step": 4620
    },
    {
      "epoch": 1.2082463465553235,
      "grad_norm": 1.2995491027832031,
      "learning_rate": 0.00012037281861448968,
      "loss": 1.7035,
      "step": 4630
    },
    {
      "epoch": 1.210855949895616,
      "grad_norm": 1.3193423748016357,
      "learning_rate": 0.00011997620306716023,
      "loss": 1.6378,
      "step": 4640
    },
    {
      "epoch": 1.2134655532359082,
      "grad_norm": 1.6104108095169067,
      "learning_rate": 0.00011957958751983077,
      "loss": 1.6381,
      "step": 4650
    },
    {
      "epoch": 1.2160751565762005,
      "grad_norm": 1.3893650770187378,
      "learning_rate": 0.00011918297197250132,
      "loss": 1.5938,
      "step": 4660
    },
    {
      "epoch": 1.2186847599164927,
      "grad_norm": 1.1101380586624146,
      "learning_rate": 0.00011878635642517186,
      "loss": 1.628,
      "step": 4670
    },
    {
      "epoch": 1.221294363256785,
      "grad_norm": 1.6506680250167847,
      "learning_rate": 0.00011838974087784239,
      "loss": 1.6763,
      "step": 4680
    },
    {
      "epoch": 1.2239039665970772,
      "grad_norm": 1.4959696531295776,
      "learning_rate": 0.00011799312533051294,
      "loss": 1.3757,
      "step": 4690
    },
    {
      "epoch": 1.2265135699373695,
      "grad_norm": 1.018555760383606,
      "learning_rate": 0.00011759650978318348,
      "loss": 1.7535,
      "step": 4700
    },
    {
      "epoch": 1.2291231732776617,
      "grad_norm": 1.2235335111618042,
      "learning_rate": 0.00011719989423585403,
      "loss": 1.5933,
      "step": 4710
    },
    {
      "epoch": 1.231732776617954,
      "grad_norm": 1.219053030014038,
      "learning_rate": 0.00011680327868852458,
      "loss": 1.7554,
      "step": 4720
    },
    {
      "epoch": 1.2343423799582462,
      "grad_norm": 1.2131617069244385,
      "learning_rate": 0.00011640666314119512,
      "loss": 1.5881,
      "step": 4730
    },
    {
      "epoch": 1.2369519832985385,
      "grad_norm": 1.4969369173049927,
      "learning_rate": 0.00011601004759386567,
      "loss": 1.5211,
      "step": 4740
    },
    {
      "epoch": 1.239561586638831,
      "grad_norm": 1.3210828304290771,
      "learning_rate": 0.00011561343204653621,
      "loss": 1.5325,
      "step": 4750
    },
    {
      "epoch": 1.2421711899791232,
      "grad_norm": 1.3608314990997314,
      "learning_rate": 0.00011521681649920676,
      "loss": 1.4916,
      "step": 4760
    },
    {
      "epoch": 1.2447807933194155,
      "grad_norm": 1.3064512014389038,
      "learning_rate": 0.0001148202009518773,
      "loss": 1.6453,
      "step": 4770
    },
    {
      "epoch": 1.2473903966597077,
      "grad_norm": 1.46491539478302,
      "learning_rate": 0.00011442358540454785,
      "loss": 1.5793,
      "step": 4780
    },
    {
      "epoch": 1.25,
      "grad_norm": 1.3472120761871338,
      "learning_rate": 0.0001140269698572184,
      "loss": 1.6385,
      "step": 4790
    },
    {
      "epoch": 1.2526096033402923,
      "grad_norm": 1.2995805740356445,
      "learning_rate": 0.00011363035430988894,
      "loss": 1.6462,
      "step": 4800
    },
    {
      "epoch": 1.2526096033402923,
      "eval_loss": 1.684793472290039,
      "eval_runtime": 1.957,
      "eval_samples_per_second": 61.319,
      "eval_steps_per_second": 7.665,
      "step": 4800
    },
    {
      "epoch": 1.2552192066805845,
      "grad_norm": 1.2800681591033936,
      "learning_rate": 0.00011323373876255949,
      "loss": 1.6531,
      "step": 4810
    },
    {
      "epoch": 1.2578288100208768,
      "grad_norm": 1.3910388946533203,
      "learning_rate": 0.00011283712321523003,
      "loss": 1.504,
      "step": 4820
    },
    {
      "epoch": 1.260438413361169,
      "grad_norm": 1.2048271894454956,
      "learning_rate": 0.00011244050766790057,
      "loss": 1.6122,
      "step": 4830
    },
    {
      "epoch": 1.2630480167014615,
      "grad_norm": 1.4826860427856445,
      "learning_rate": 0.00011204389212057111,
      "loss": 1.5912,
      "step": 4840
    },
    {
      "epoch": 1.2656576200417535,
      "grad_norm": 1.6443843841552734,
      "learning_rate": 0.00011164727657324165,
      "loss": 1.4847,
      "step": 4850
    },
    {
      "epoch": 1.268267223382046,
      "grad_norm": 1.5175312757492065,
      "learning_rate": 0.0001112506610259122,
      "loss": 1.3656,
      "step": 4860
    },
    {
      "epoch": 1.2708768267223383,
      "grad_norm": 1.356987714767456,
      "learning_rate": 0.00011085404547858275,
      "loss": 1.5959,
      "step": 4870
    },
    {
      "epoch": 1.2734864300626305,
      "grad_norm": 1.0908865928649902,
      "learning_rate": 0.0001104574299312533,
      "loss": 1.6905,
      "step": 4880
    },
    {
      "epoch": 1.2760960334029228,
      "grad_norm": 1.1202874183654785,
      "learning_rate": 0.00011006081438392384,
      "loss": 1.5898,
      "step": 4890
    },
    {
      "epoch": 1.278705636743215,
      "grad_norm": 1.8282641172409058,
      "learning_rate": 0.00010966419883659439,
      "loss": 1.7482,
      "step": 4900
    },
    {
      "epoch": 1.2813152400835073,
      "grad_norm": 1.167696237564087,
      "learning_rate": 0.00010926758328926493,
      "loss": 1.7301,
      "step": 4910
    },
    {
      "epoch": 1.2839248434237995,
      "grad_norm": 1.3167637586593628,
      "learning_rate": 0.00010887096774193548,
      "loss": 1.4794,
      "step": 4920
    },
    {
      "epoch": 1.2865344467640918,
      "grad_norm": 1.5598312616348267,
      "learning_rate": 0.00010847435219460602,
      "loss": 1.5239,
      "step": 4930
    },
    {
      "epoch": 1.289144050104384,
      "grad_norm": 1.4839824438095093,
      "learning_rate": 0.00010807773664727657,
      "loss": 1.5968,
      "step": 4940
    },
    {
      "epoch": 1.2917536534446765,
      "grad_norm": 1.2524480819702148,
      "learning_rate": 0.00010768112109994712,
      "loss": 1.6218,
      "step": 4950
    },
    {
      "epoch": 1.2943632567849686,
      "grad_norm": 1.3225451707839966,
      "learning_rate": 0.00010728450555261766,
      "loss": 1.5791,
      "step": 4960
    },
    {
      "epoch": 1.296972860125261,
      "grad_norm": 1.4639559984207153,
      "learning_rate": 0.00010688789000528819,
      "loss": 1.5258,
      "step": 4970
    },
    {
      "epoch": 1.2995824634655533,
      "grad_norm": 1.6156091690063477,
      "learning_rate": 0.00010649127445795874,
      "loss": 1.5081,
      "step": 4980
    },
    {
      "epoch": 1.3021920668058455,
      "grad_norm": 1.6682195663452148,
      "learning_rate": 0.00010609465891062928,
      "loss": 1.5507,
      "step": 4990
    },
    {
      "epoch": 1.3048016701461378,
      "grad_norm": 1.2938944101333618,
      "learning_rate": 0.00010569804336329982,
      "loss": 1.5011,
      "step": 5000
    },
    {
      "epoch": 1.3048016701461378,
      "eval_loss": 1.6789017915725708,
      "eval_runtime": 1.9624,
      "eval_samples_per_second": 61.148,
      "eval_steps_per_second": 7.644,
      "step": 5000
    },
    {
      "epoch": 1.30741127348643,
      "grad_norm": 1.1716530323028564,
      "learning_rate": 0.00010530142781597038,
      "loss": 1.7471,
      "step": 5010
    },
    {
      "epoch": 1.3100208768267223,
      "grad_norm": 1.3529021739959717,
      "learning_rate": 0.00010490481226864092,
      "loss": 1.7298,
      "step": 5020
    },
    {
      "epoch": 1.3126304801670146,
      "grad_norm": 1.155781626701355,
      "learning_rate": 0.00010450819672131146,
      "loss": 1.7272,
      "step": 5030
    },
    {
      "epoch": 1.3152400835073068,
      "grad_norm": 1.515201449394226,
      "learning_rate": 0.000104111581173982,
      "loss": 1.725,
      "step": 5040
    },
    {
      "epoch": 1.317849686847599,
      "grad_norm": 1.1768853664398193,
      "learning_rate": 0.00010371496562665256,
      "loss": 1.6245,
      "step": 5050
    },
    {
      "epoch": 1.3204592901878915,
      "grad_norm": 1.3241647481918335,
      "learning_rate": 0.0001033183500793231,
      "loss": 1.6183,
      "step": 5060
    },
    {
      "epoch": 1.3230688935281838,
      "grad_norm": 1.1881295442581177,
      "learning_rate": 0.00010292173453199365,
      "loss": 1.6812,
      "step": 5070
    },
    {
      "epoch": 1.325678496868476,
      "grad_norm": 2.34753680229187,
      "learning_rate": 0.0001025251189846642,
      "loss": 1.622,
      "step": 5080
    },
    {
      "epoch": 1.3282881002087683,
      "grad_norm": 1.0019495487213135,
      "learning_rate": 0.00010212850343733474,
      "loss": 1.4945,
      "step": 5090
    },
    {
      "epoch": 1.3308977035490606,
      "grad_norm": 1.2635695934295654,
      "learning_rate": 0.00010173188789000529,
      "loss": 1.4395,
      "step": 5100
    },
    {
      "epoch": 1.3335073068893528,
      "grad_norm": 1.341418743133545,
      "learning_rate": 0.00010133527234267581,
      "loss": 1.6311,
      "step": 5110
    },
    {
      "epoch": 1.336116910229645,
      "grad_norm": 1.0141221284866333,
      "learning_rate": 0.00010093865679534636,
      "loss": 1.5976,
      "step": 5120
    },
    {
      "epoch": 1.3387265135699373,
      "grad_norm": 1.5653742551803589,
      "learning_rate": 0.00010054204124801691,
      "loss": 1.7203,
      "step": 5130
    },
    {
      "epoch": 1.3413361169102296,
      "grad_norm": 1.8261111974716187,
      "learning_rate": 0.00010014542570068745,
      "loss": 1.6942,
      "step": 5140
    },
    {
      "epoch": 1.343945720250522,
      "grad_norm": 1.4081995487213135,
      "learning_rate": 9.9748810153358e-05,
      "loss": 1.6646,
      "step": 5150
    },
    {
      "epoch": 1.346555323590814,
      "grad_norm": 1.2043793201446533,
      "learning_rate": 9.935219460602855e-05,
      "loss": 1.5904,
      "step": 5160
    },
    {
      "epoch": 1.3491649269311066,
      "grad_norm": 1.2501511573791504,
      "learning_rate": 9.895557905869909e-05,
      "loss": 1.6466,
      "step": 5170
    },
    {
      "epoch": 1.3517745302713988,
      "grad_norm": 1.733376145362854,
      "learning_rate": 9.855896351136963e-05,
      "loss": 1.6034,
      "step": 5180
    },
    {
      "epoch": 1.354384133611691,
      "grad_norm": 1.3947391510009766,
      "learning_rate": 9.816234796404018e-05,
      "loss": 1.6989,
      "step": 5190
    },
    {
      "epoch": 1.3569937369519833,
      "grad_norm": 1.7693610191345215,
      "learning_rate": 9.776573241671073e-05,
      "loss": 1.5809,
      "step": 5200
    },
    {
      "epoch": 1.3569937369519833,
      "eval_loss": 1.6735635995864868,
      "eval_runtime": 1.9562,
      "eval_samples_per_second": 61.343,
      "eval_steps_per_second": 7.668,
      "step": 5200
    },
    {
      "epoch": 1.3596033402922756,
      "grad_norm": 1.3579843044281006,
      "learning_rate": 9.736911686938127e-05,
      "loss": 1.7389,
      "step": 5210
    },
    {
      "epoch": 1.3622129436325678,
      "grad_norm": 1.5082885026931763,
      "learning_rate": 9.697250132205182e-05,
      "loss": 1.7549,
      "step": 5220
    },
    {
      "epoch": 1.36482254697286,
      "grad_norm": 1.4587095975875854,
      "learning_rate": 9.657588577472237e-05,
      "loss": 1.5887,
      "step": 5230
    },
    {
      "epoch": 1.3674321503131524,
      "grad_norm": 0.9745476245880127,
      "learning_rate": 9.617927022739291e-05,
      "loss": 1.5218,
      "step": 5240
    },
    {
      "epoch": 1.3700417536534446,
      "grad_norm": 1.5976052284240723,
      "learning_rate": 9.578265468006344e-05,
      "loss": 1.7141,
      "step": 5250
    },
    {
      "epoch": 1.372651356993737,
      "grad_norm": 1.3669447898864746,
      "learning_rate": 9.538603913273398e-05,
      "loss": 1.8578,
      "step": 5260
    },
    {
      "epoch": 1.3752609603340291,
      "grad_norm": 1.3490023612976074,
      "learning_rate": 9.498942358540454e-05,
      "loss": 1.5814,
      "step": 5270
    },
    {
      "epoch": 1.3778705636743216,
      "grad_norm": 1.227745771408081,
      "learning_rate": 9.459280803807508e-05,
      "loss": 1.4501,
      "step": 5280
    },
    {
      "epoch": 1.3804801670146138,
      "grad_norm": 1.1860342025756836,
      "learning_rate": 9.419619249074562e-05,
      "loss": 1.7024,
      "step": 5290
    },
    {
      "epoch": 1.383089770354906,
      "grad_norm": 1.3313522338867188,
      "learning_rate": 9.379957694341617e-05,
      "loss": 1.6163,
      "step": 5300
    },
    {
      "epoch": 1.3856993736951984,
      "grad_norm": 1.2202141284942627,
      "learning_rate": 9.340296139608672e-05,
      "loss": 1.4886,
      "step": 5310
    },
    {
      "epoch": 1.3883089770354906,
      "grad_norm": 1.6506894826889038,
      "learning_rate": 9.300634584875726e-05,
      "loss": 1.6718,
      "step": 5320
    },
    {
      "epoch": 1.3909185803757829,
      "grad_norm": 1.3435816764831543,
      "learning_rate": 9.26097303014278e-05,
      "loss": 1.4386,
      "step": 5330
    },
    {
      "epoch": 1.3935281837160751,
      "grad_norm": 1.3367422819137573,
      "learning_rate": 9.221311475409836e-05,
      "loss": 1.6397,
      "step": 5340
    },
    {
      "epoch": 1.3961377870563674,
      "grad_norm": 1.2440458536148071,
      "learning_rate": 9.18164992067689e-05,
      "loss": 1.4222,
      "step": 5350
    },
    {
      "epoch": 1.3987473903966596,
      "grad_norm": 1.1080514192581177,
      "learning_rate": 9.141988365943944e-05,
      "loss": 1.7905,
      "step": 5360
    },
    {
      "epoch": 1.401356993736952,
      "grad_norm": 1.4248647689819336,
      "learning_rate": 9.102326811210999e-05,
      "loss": 1.4987,
      "step": 5370
    },
    {
      "epoch": 1.4039665970772441,
      "grad_norm": 1.4987988471984863,
      "learning_rate": 9.062665256478054e-05,
      "loss": 1.4924,
      "step": 5380
    },
    {
      "epoch": 1.4065762004175366,
      "grad_norm": 1.5507434606552124,
      "learning_rate": 9.023003701745108e-05,
      "loss": 1.6548,
      "step": 5390
    },
    {
      "epoch": 1.4091858037578289,
      "grad_norm": 1.4161410331726074,
      "learning_rate": 8.983342147012161e-05,
      "loss": 1.6689,
      "step": 5400
    },
    {
      "epoch": 1.4091858037578289,
      "eval_loss": 1.6671433448791504,
      "eval_runtime": 1.9606,
      "eval_samples_per_second": 61.206,
      "eval_steps_per_second": 7.651,
      "step": 5400
    },
    {
      "epoch": 1.4117954070981211,
      "grad_norm": 2.000509262084961,
      "learning_rate": 8.943680592279215e-05,
      "loss": 1.6095,
      "step": 5410
    },
    {
      "epoch": 1.4144050104384134,
      "grad_norm": 1.2577446699142456,
      "learning_rate": 8.904019037546271e-05,
      "loss": 1.7894,
      "step": 5420
    },
    {
      "epoch": 1.4170146137787056,
      "grad_norm": 1.6913281679153442,
      "learning_rate": 8.864357482813325e-05,
      "loss": 1.5778,
      "step": 5430
    },
    {
      "epoch": 1.4196242171189979,
      "grad_norm": 1.3570008277893066,
      "learning_rate": 8.82469592808038e-05,
      "loss": 1.5292,
      "step": 5440
    },
    {
      "epoch": 1.4222338204592901,
      "grad_norm": 1.7011005878448486,
      "learning_rate": 8.785034373347435e-05,
      "loss": 1.7472,
      "step": 5450
    },
    {
      "epoch": 1.4248434237995824,
      "grad_norm": 1.5350279808044434,
      "learning_rate": 8.745372818614489e-05,
      "loss": 1.661,
      "step": 5460
    },
    {
      "epoch": 1.4274530271398747,
      "grad_norm": 1.4327185153961182,
      "learning_rate": 8.705711263881543e-05,
      "loss": 1.5913,
      "step": 5470
    },
    {
      "epoch": 1.4300626304801671,
      "grad_norm": 1.1708102226257324,
      "learning_rate": 8.666049709148598e-05,
      "loss": 1.6266,
      "step": 5480
    },
    {
      "epoch": 1.4326722338204592,
      "grad_norm": 1.426356315612793,
      "learning_rate": 8.626388154415653e-05,
      "loss": 1.551,
      "step": 5490
    },
    {
      "epoch": 1.4352818371607516,
      "grad_norm": 1.2362685203552246,
      "learning_rate": 8.586726599682707e-05,
      "loss": 1.7022,
      "step": 5500
    },
    {
      "epoch": 1.437891440501044,
      "grad_norm": 1.9705733060836792,
      "learning_rate": 8.547065044949761e-05,
      "loss": 1.6694,
      "step": 5510
    },
    {
      "epoch": 1.4405010438413361,
      "grad_norm": 1.1712028980255127,
      "learning_rate": 8.507403490216817e-05,
      "loss": 1.5325,
      "step": 5520
    },
    {
      "epoch": 1.4431106471816284,
      "grad_norm": 1.5681546926498413,
      "learning_rate": 8.467741935483871e-05,
      "loss": 1.7256,
      "step": 5530
    },
    {
      "epoch": 1.4457202505219207,
      "grad_norm": 1.3759323358535767,
      "learning_rate": 8.428080380750924e-05,
      "loss": 1.6244,
      "step": 5540
    },
    {
      "epoch": 1.448329853862213,
      "grad_norm": 1.2528462409973145,
      "learning_rate": 8.388418826017978e-05,
      "loss": 1.7705,
      "step": 5550
    },
    {
      "epoch": 1.4509394572025052,
      "grad_norm": 1.175582766532898,
      "learning_rate": 8.348757271285032e-05,
      "loss": 1.5405,
      "step": 5560
    },
    {
      "epoch": 1.4535490605427974,
      "grad_norm": 1.0067615509033203,
      "learning_rate": 8.309095716552088e-05,
      "loss": 1.6095,
      "step": 5570
    },
    {
      "epoch": 1.4561586638830897,
      "grad_norm": 1.6234732866287231,
      "learning_rate": 8.269434161819142e-05,
      "loss": 1.621,
      "step": 5580
    },
    {
      "epoch": 1.4587682672233822,
      "grad_norm": 1.5708647966384888,
      "learning_rate": 8.229772607086196e-05,
      "loss": 1.3778,
      "step": 5590
    },
    {
      "epoch": 1.4613778705636742,
      "grad_norm": 1.140479326248169,
      "learning_rate": 8.190111052353252e-05,
      "loss": 1.5451,
      "step": 5600
    },
    {
      "epoch": 1.4613778705636742,
      "eval_loss": 1.662474274635315,
      "eval_runtime": 1.9613,
      "eval_samples_per_second": 61.182,
      "eval_steps_per_second": 7.648,
      "step": 5600
    },
    {
      "epoch": 1.4639874739039667,
      "grad_norm": 1.9424175024032593,
      "learning_rate": 8.150449497620306e-05,
      "loss": 1.8256,
      "step": 5610
    },
    {
      "epoch": 1.466597077244259,
      "grad_norm": 1.4428598880767822,
      "learning_rate": 8.11078794288736e-05,
      "loss": 1.8046,
      "step": 5620
    },
    {
      "epoch": 1.4692066805845512,
      "grad_norm": 1.2788069248199463,
      "learning_rate": 8.071126388154416e-05,
      "loss": 1.6696,
      "step": 5630
    },
    {
      "epoch": 1.4718162839248434,
      "grad_norm": 1.168588638305664,
      "learning_rate": 8.03146483342147e-05,
      "loss": 1.5624,
      "step": 5640
    },
    {
      "epoch": 1.4744258872651357,
      "grad_norm": 1.2837905883789062,
      "learning_rate": 7.991803278688524e-05,
      "loss": 1.6582,
      "step": 5650
    },
    {
      "epoch": 1.477035490605428,
      "grad_norm": 1.4767245054244995,
      "learning_rate": 7.952141723955579e-05,
      "loss": 1.5642,
      "step": 5660
    },
    {
      "epoch": 1.4796450939457202,
      "grad_norm": 1.6881823539733887,
      "learning_rate": 7.912480169222634e-05,
      "loss": 1.5376,
      "step": 5670
    },
    {
      "epoch": 1.4822546972860124,
      "grad_norm": 1.2603791952133179,
      "learning_rate": 7.872818614489687e-05,
      "loss": 1.5983,
      "step": 5680
    },
    {
      "epoch": 1.4848643006263047,
      "grad_norm": 1.3706929683685303,
      "learning_rate": 7.833157059756741e-05,
      "loss": 1.4403,
      "step": 5690
    },
    {
      "epoch": 1.4874739039665972,
      "grad_norm": 1.2359133958816528,
      "learning_rate": 7.793495505023795e-05,
      "loss": 1.6941,
      "step": 5700
    },
    {
      "epoch": 1.4900835073068894,
      "grad_norm": 1.538825273513794,
      "learning_rate": 7.757800105764145e-05,
      "loss": 1.4504,
      "step": 5710
    },
    {
      "epoch": 1.4926931106471817,
      "grad_norm": 1.2529953718185425,
      "learning_rate": 7.7181385510312e-05,
      "loss": 1.5332,
      "step": 5720
    },
    {
      "epoch": 1.495302713987474,
      "grad_norm": 1.4256794452667236,
      "learning_rate": 7.678476996298255e-05,
      "loss": 1.658,
      "step": 5730
    },
    {
      "epoch": 1.4979123173277662,
      "grad_norm": 1.3858290910720825,
      "learning_rate": 7.638815441565309e-05,
      "loss": 1.5623,
      "step": 5740
    },
    {
      "epoch": 1.5005219206680585,
      "grad_norm": 4.521997451782227,
      "learning_rate": 7.599153886832363e-05,
      "loss": 1.7263,
      "step": 5750
    },
    {
      "epoch": 1.5031315240083507,
      "grad_norm": 1.284297227859497,
      "learning_rate": 7.559492332099419e-05,
      "loss": 1.4178,
      "step": 5760
    },
    {
      "epoch": 1.505741127348643,
      "grad_norm": 1.3344568014144897,
      "learning_rate": 7.519830777366473e-05,
      "loss": 1.8259,
      "step": 5770
    },
    {
      "epoch": 1.5083507306889352,
      "grad_norm": 1.0959270000457764,
      "learning_rate": 7.480169222633527e-05,
      "loss": 1.4855,
      "step": 5780
    },
    {
      "epoch": 1.5109603340292277,
      "grad_norm": 1.9954848289489746,
      "learning_rate": 7.440507667900581e-05,
      "loss": 1.6736,
      "step": 5790
    },
    {
      "epoch": 1.5135699373695197,
      "grad_norm": 1.3153899908065796,
      "learning_rate": 7.400846113167636e-05,
      "loss": 1.6139,
      "step": 5800
    },
    {
      "epoch": 1.5135699373695197,
      "eval_loss": 1.6588425636291504,
      "eval_runtime": 1.9598,
      "eval_samples_per_second": 61.231,
      "eval_steps_per_second": 7.654,
      "step": 5800
    },
    {
      "epoch": 1.5161795407098122,
      "grad_norm": 1.777503252029419,
      "learning_rate": 7.361184558434691e-05,
      "loss": 1.4074,
      "step": 5810
    },
    {
      "epoch": 1.5187891440501042,
      "grad_norm": 1.272653341293335,
      "learning_rate": 7.321523003701744e-05,
      "loss": 1.4696,
      "step": 5820
    },
    {
      "epoch": 1.5213987473903967,
      "grad_norm": 1.3065693378448486,
      "learning_rate": 7.281861448968798e-05,
      "loss": 1.5581,
      "step": 5830
    },
    {
      "epoch": 1.524008350730689,
      "grad_norm": 1.4888718128204346,
      "learning_rate": 7.242199894235854e-05,
      "loss": 1.5271,
      "step": 5840
    },
    {
      "epoch": 1.5266179540709812,
      "grad_norm": 1.1870439052581787,
      "learning_rate": 7.202538339502908e-05,
      "loss": 1.4878,
      "step": 5850
    },
    {
      "epoch": 1.5292275574112735,
      "grad_norm": 2.043999195098877,
      "learning_rate": 7.162876784769962e-05,
      "loss": 1.5216,
      "step": 5860
    },
    {
      "epoch": 1.5318371607515657,
      "grad_norm": 1.3523449897766113,
      "learning_rate": 7.123215230037018e-05,
      "loss": 1.7931,
      "step": 5870
    },
    {
      "epoch": 1.534446764091858,
      "grad_norm": 1.804303765296936,
      "learning_rate": 7.083553675304072e-05,
      "loss": 1.42,
      "step": 5880
    },
    {
      "epoch": 1.5370563674321502,
      "grad_norm": 1.2895362377166748,
      "learning_rate": 7.043892120571126e-05,
      "loss": 1.6162,
      "step": 5890
    },
    {
      "epoch": 1.5396659707724427,
      "grad_norm": 1.1331487894058228,
      "learning_rate": 7.00423056583818e-05,
      "loss": 1.5765,
      "step": 5900
    },
    {
      "epoch": 1.5422755741127347,
      "grad_norm": 1.5976550579071045,
      "learning_rate": 6.964569011105234e-05,
      "loss": 1.5534,
      "step": 5910
    },
    {
      "epoch": 1.5448851774530272,
      "grad_norm": 1.825168251991272,
      "learning_rate": 6.924907456372289e-05,
      "loss": 1.8393,
      "step": 5920
    },
    {
      "epoch": 1.5474947807933193,
      "grad_norm": 1.14243483543396,
      "learning_rate": 6.885245901639344e-05,
      "loss": 1.5732,
      "step": 5930
    },
    {
      "epoch": 1.5501043841336117,
      "grad_norm": 1.289762020111084,
      "learning_rate": 6.845584346906398e-05,
      "loss": 1.5431,
      "step": 5940
    },
    {
      "epoch": 1.552713987473904,
      "grad_norm": 1.383611798286438,
      "learning_rate": 6.805922792173453e-05,
      "loss": 1.48,
      "step": 5950
    },
    {
      "epoch": 1.5553235908141962,
      "grad_norm": 1.34585702419281,
      "learning_rate": 6.766261237440507e-05,
      "loss": 1.5489,
      "step": 5960
    },
    {
      "epoch": 1.5579331941544885,
      "grad_norm": 1.3675017356872559,
      "learning_rate": 6.726599682707561e-05,
      "loss": 1.63,
      "step": 5970
    },
    {
      "epoch": 1.5605427974947808,
      "grad_norm": 1.4266268014907837,
      "learning_rate": 6.686938127974616e-05,
      "loss": 1.7366,
      "step": 5980
    },
    {
      "epoch": 1.5631524008350732,
      "grad_norm": 1.6202846765518188,
      "learning_rate": 6.647276573241671e-05,
      "loss": 1.6565,
      "step": 5990
    },
    {
      "epoch": 1.5657620041753653,
      "grad_norm": 1.3544055223464966,
      "learning_rate": 6.607615018508725e-05,
      "loss": 1.729,
      "step": 6000
    },
    {
      "epoch": 1.5657620041753653,
      "eval_loss": 1.6550192832946777,
      "eval_runtime": 1.962,
      "eval_samples_per_second": 61.162,
      "eval_steps_per_second": 7.645,
      "step": 6000
    },
    {
      "epoch": 1.5683716075156577,
      "grad_norm": 1.0465859174728394,
      "learning_rate": 6.567953463775779e-05,
      "loss": 1.6941,
      "step": 6010
    },
    {
      "epoch": 1.5709812108559498,
      "grad_norm": 1.4627747535705566,
      "learning_rate": 6.528291909042835e-05,
      "loss": 1.629,
      "step": 6020
    },
    {
      "epoch": 1.5735908141962422,
      "grad_norm": 1.7565668821334839,
      "learning_rate": 6.488630354309887e-05,
      "loss": 1.5911,
      "step": 6030
    },
    {
      "epoch": 1.5762004175365343,
      "grad_norm": 1.5590829849243164,
      "learning_rate": 6.448968799576943e-05,
      "loss": 1.4766,
      "step": 6040
    },
    {
      "epoch": 1.5788100208768268,
      "grad_norm": 1.735615611076355,
      "learning_rate": 6.409307244843997e-05,
      "loss": 1.646,
      "step": 6050
    },
    {
      "epoch": 1.581419624217119,
      "grad_norm": 1.2695159912109375,
      "learning_rate": 6.369645690111051e-05,
      "loss": 1.5124,
      "step": 6060
    },
    {
      "epoch": 1.5840292275574113,
      "grad_norm": 1.794276237487793,
      "learning_rate": 6.329984135378107e-05,
      "loss": 1.6404,
      "step": 6070
    },
    {
      "epoch": 1.5866388308977035,
      "grad_norm": 1.2704170942306519,
      "learning_rate": 6.290322580645161e-05,
      "loss": 1.6459,
      "step": 6080
    },
    {
      "epoch": 1.5892484342379958,
      "grad_norm": 1.2704652547836304,
      "learning_rate": 6.250661025912215e-05,
      "loss": 1.489,
      "step": 6090
    },
    {
      "epoch": 1.5918580375782883,
      "grad_norm": 1.47409188747406,
      "learning_rate": 6.21099947117927e-05,
      "loss": 1.6482,
      "step": 6100
    },
    {
      "epoch": 1.5944676409185803,
      "grad_norm": 1.3320717811584473,
      "learning_rate": 6.171337916446324e-05,
      "loss": 1.6337,
      "step": 6110
    },
    {
      "epoch": 1.5970772442588728,
      "grad_norm": 1.363405466079712,
      "learning_rate": 6.131676361713378e-05,
      "loss": 1.6487,
      "step": 6120
    },
    {
      "epoch": 1.5996868475991648,
      "grad_norm": 1.3707727193832397,
      "learning_rate": 6.092014806980433e-05,
      "loss": 1.4301,
      "step": 6130
    },
    {
      "epoch": 1.6022964509394573,
      "grad_norm": 1.2330065965652466,
      "learning_rate": 6.052353252247488e-05,
      "loss": 1.6236,
      "step": 6140
    },
    {
      "epoch": 1.6049060542797495,
      "grad_norm": 1.20502769947052,
      "learning_rate": 6.0126916975145426e-05,
      "loss": 1.4879,
      "step": 6150
    },
    {
      "epoch": 1.6075156576200418,
      "grad_norm": 1.0251761674880981,
      "learning_rate": 5.973030142781597e-05,
      "loss": 1.6461,
      "step": 6160
    },
    {
      "epoch": 1.610125260960334,
      "grad_norm": 1.7741361856460571,
      "learning_rate": 5.93336858804865e-05,
      "loss": 1.5015,
      "step": 6170
    },
    {
      "epoch": 1.6127348643006263,
      "grad_norm": 1.2082395553588867,
      "learning_rate": 5.893707033315705e-05,
      "loss": 1.6254,
      "step": 6180
    },
    {
      "epoch": 1.6153444676409185,
      "grad_norm": 1.425494909286499,
      "learning_rate": 5.85404547858276e-05,
      "loss": 1.7594,
      "step": 6190
    },
    {
      "epoch": 1.6179540709812108,
      "grad_norm": 1.273876428604126,
      "learning_rate": 5.814383923849814e-05,
      "loss": 1.6252,
      "step": 6200
    },
    {
      "epoch": 1.6179540709812108,
      "eval_loss": 1.651541829109192,
      "eval_runtime": 1.9593,
      "eval_samples_per_second": 61.248,
      "eval_steps_per_second": 7.656,
      "step": 6200
    },
    {
      "epoch": 1.6205636743215033,
      "grad_norm": 1.1941546201705933,
      "learning_rate": 5.774722369116869e-05,
      "loss": 1.4472,
      "step": 6210
    },
    {
      "epoch": 1.6231732776617953,
      "grad_norm": 1.4497381448745728,
      "learning_rate": 5.735060814383923e-05,
      "loss": 1.6108,
      "step": 6220
    },
    {
      "epoch": 1.6257828810020878,
      "grad_norm": 1.3225960731506348,
      "learning_rate": 5.695399259650978e-05,
      "loss": 1.5392,
      "step": 6230
    },
    {
      "epoch": 1.6283924843423798,
      "grad_norm": 1.5613771677017212,
      "learning_rate": 5.655737704918032e-05,
      "loss": 1.5893,
      "step": 6240
    },
    {
      "epoch": 1.6310020876826723,
      "grad_norm": 0.7834975719451904,
      "learning_rate": 5.6160761501850866e-05,
      "loss": 1.514,
      "step": 6250
    },
    {
      "epoch": 1.6336116910229646,
      "grad_norm": 1.7471693754196167,
      "learning_rate": 5.576414595452141e-05,
      "loss": 1.5449,
      "step": 6260
    },
    {
      "epoch": 1.6362212943632568,
      "grad_norm": 1.4879164695739746,
      "learning_rate": 5.536753040719196e-05,
      "loss": 1.6115,
      "step": 6270
    },
    {
      "epoch": 1.638830897703549,
      "grad_norm": 1.4260367155075073,
      "learning_rate": 5.4970914859862505e-05,
      "loss": 1.5105,
      "step": 6280
    },
    {
      "epoch": 1.6414405010438413,
      "grad_norm": 1.3694658279418945,
      "learning_rate": 5.457429931253305e-05,
      "loss": 1.5702,
      "step": 6290
    },
    {
      "epoch": 1.6440501043841336,
      "grad_norm": 1.9584527015686035,
      "learning_rate": 5.4177683765203596e-05,
      "loss": 1.771,
      "step": 6300
    },
    {
      "epoch": 1.6466597077244258,
      "grad_norm": 1.3805478811264038,
      "learning_rate": 5.378106821787413e-05,
      "loss": 1.6771,
      "step": 6310
    },
    {
      "epoch": 1.6492693110647183,
      "grad_norm": 1.5223937034606934,
      "learning_rate": 5.338445267054468e-05,
      "loss": 1.5806,
      "step": 6320
    },
    {
      "epoch": 1.6518789144050103,
      "grad_norm": 1.2875701189041138,
      "learning_rate": 5.298783712321522e-05,
      "loss": 1.8392,
      "step": 6330
    },
    {
      "epoch": 1.6544885177453028,
      "grad_norm": 1.2219221591949463,
      "learning_rate": 5.259122157588577e-05,
      "loss": 1.7935,
      "step": 6340
    },
    {
      "epoch": 1.6570981210855948,
      "grad_norm": 1.506008267402649,
      "learning_rate": 5.219460602855631e-05,
      "loss": 1.4556,
      "step": 6350
    },
    {
      "epoch": 1.6597077244258873,
      "grad_norm": 1.5929179191589355,
      "learning_rate": 5.179799048122686e-05,
      "loss": 1.4481,
      "step": 6360
    },
    {
      "epoch": 1.6623173277661796,
      "grad_norm": 1.5365301370620728,
      "learning_rate": 5.140137493389741e-05,
      "loss": 1.5668,
      "step": 6370
    },
    {
      "epoch": 1.6649269311064718,
      "grad_norm": 1.5211595296859741,
      "learning_rate": 5.1004759386567946e-05,
      "loss": 1.4024,
      "step": 6380
    },
    {
      "epoch": 1.667536534446764,
      "grad_norm": 1.203516960144043,
      "learning_rate": 5.060814383923849e-05,
      "loss": 1.4595,
      "step": 6390
    },
    {
      "epoch": 1.6701461377870563,
      "grad_norm": 1.3924200534820557,
      "learning_rate": 5.0211528291909036e-05,
      "loss": 1.6417,
      "step": 6400
    },
    {
      "epoch": 1.6701461377870563,
      "eval_loss": 1.6482932567596436,
      "eval_runtime": 1.965,
      "eval_samples_per_second": 61.069,
      "eval_steps_per_second": 7.634,
      "step": 6400
    },
    {
      "epoch": 1.6727557411273486,
      "grad_norm": 1.937188982963562,
      "learning_rate": 4.9814912744579585e-05,
      "loss": 1.7435,
      "step": 6410
    },
    {
      "epoch": 1.6753653444676408,
      "grad_norm": 1.905369520187378,
      "learning_rate": 4.941829719725013e-05,
      "loss": 1.666,
      "step": 6420
    },
    {
      "epoch": 1.6779749478079333,
      "grad_norm": 1.933645248413086,
      "learning_rate": 4.9021681649920676e-05,
      "loss": 1.7281,
      "step": 6430
    },
    {
      "epoch": 1.6805845511482254,
      "grad_norm": 1.1868964433670044,
      "learning_rate": 4.862506610259122e-05,
      "loss": 1.3311,
      "step": 6440
    },
    {
      "epoch": 1.6831941544885178,
      "grad_norm": 1.431754469871521,
      "learning_rate": 4.8228450555261766e-05,
      "loss": 1.4086,
      "step": 6450
    },
    {
      "epoch": 1.6858037578288099,
      "grad_norm": 1.1171798706054688,
      "learning_rate": 4.78318350079323e-05,
      "loss": 1.666,
      "step": 6460
    },
    {
      "epoch": 1.6884133611691023,
      "grad_norm": 1.5543972253799438,
      "learning_rate": 4.743521946060285e-05,
      "loss": 1.6882,
      "step": 6470
    },
    {
      "epoch": 1.6910229645093946,
      "grad_norm": 1.2861136198043823,
      "learning_rate": 4.703860391327339e-05,
      "loss": 1.5608,
      "step": 6480
    },
    {
      "epoch": 1.6936325678496869,
      "grad_norm": 1.5053824186325073,
      "learning_rate": 4.664198836594394e-05,
      "loss": 1.8225,
      "step": 6490
    },
    {
      "epoch": 1.696242171189979,
      "grad_norm": 1.482244610786438,
      "learning_rate": 4.624537281861449e-05,
      "loss": 1.804,
      "step": 6500
    },
    {
      "epoch": 1.6988517745302714,
      "grad_norm": 1.2106634378433228,
      "learning_rate": 4.584875727128503e-05,
      "loss": 1.6335,
      "step": 6510
    },
    {
      "epoch": 1.7014613778705638,
      "grad_norm": 1.3635084629058838,
      "learning_rate": 4.545214172395558e-05,
      "loss": 1.668,
      "step": 6520
    },
    {
      "epoch": 1.7040709812108559,
      "grad_norm": 1.8463749885559082,
      "learning_rate": 4.5055526176626116e-05,
      "loss": 1.4397,
      "step": 6530
    },
    {
      "epoch": 1.7066805845511483,
      "grad_norm": 1.1104025840759277,
      "learning_rate": 4.4658910629296665e-05,
      "loss": 1.5859,
      "step": 6540
    },
    {
      "epoch": 1.7092901878914404,
      "grad_norm": 1.789725661277771,
      "learning_rate": 4.4262295081967207e-05,
      "loss": 1.8696,
      "step": 6550
    },
    {
      "epoch": 1.7118997912317329,
      "grad_norm": 1.2035046815872192,
      "learning_rate": 4.3865679534637755e-05,
      "loss": 1.3788,
      "step": 6560
    },
    {
      "epoch": 1.714509394572025,
      "grad_norm": 1.248658537864685,
      "learning_rate": 4.34690639873083e-05,
      "loss": 1.7087,
      "step": 6570
    },
    {
      "epoch": 1.7171189979123174,
      "grad_norm": 1.857822060585022,
      "learning_rate": 4.3072448439978846e-05,
      "loss": 1.3424,
      "step": 6580
    },
    {
      "epoch": 1.7197286012526096,
      "grad_norm": 1.5650219917297363,
      "learning_rate": 4.2675832892649395e-05,
      "loss": 1.5349,
      "step": 6590
    },
    {
      "epoch": 1.7223382045929019,
      "grad_norm": 1.7776141166687012,
      "learning_rate": 4.227921734531993e-05,
      "loss": 1.4792,
      "step": 6600
    },
    {
      "epoch": 1.7223382045929019,
      "eval_loss": 1.6433566808700562,
      "eval_runtime": 1.9585,
      "eval_samples_per_second": 61.272,
      "eval_steps_per_second": 7.659,
      "step": 6600
    },
    {
      "epoch": 1.7249478079331941,
      "grad_norm": 1.21061372756958,
      "learning_rate": 4.188260179799048e-05,
      "loss": 1.6864,
      "step": 6610
    },
    {
      "epoch": 1.7275574112734864,
      "grad_norm": 1.4900249242782593,
      "learning_rate": 4.148598625066102e-05,
      "loss": 1.652,
      "step": 6620
    },
    {
      "epoch": 1.7301670146137789,
      "grad_norm": 1.1990848779678345,
      "learning_rate": 4.108937070333157e-05,
      "loss": 1.5951,
      "step": 6630
    },
    {
      "epoch": 1.732776617954071,
      "grad_norm": 1.597954273223877,
      "learning_rate": 4.069275515600211e-05,
      "loss": 1.7028,
      "step": 6640
    },
    {
      "epoch": 1.7353862212943634,
      "grad_norm": 1.2704893350601196,
      "learning_rate": 4.029613960867266e-05,
      "loss": 1.6517,
      "step": 6650
    },
    {
      "epoch": 1.7379958246346554,
      "grad_norm": 1.5059839487075806,
      "learning_rate": 3.98995240613432e-05,
      "loss": 1.3783,
      "step": 6660
    },
    {
      "epoch": 1.7406054279749479,
      "grad_norm": 1.5861501693725586,
      "learning_rate": 3.9502908514013744e-05,
      "loss": 1.5339,
      "step": 6670
    },
    {
      "epoch": 1.7432150313152401,
      "grad_norm": 1.2301173210144043,
      "learning_rate": 3.9106292966684286e-05,
      "loss": 1.6541,
      "step": 6680
    },
    {
      "epoch": 1.7458246346555324,
      "grad_norm": 1.1716598272323608,
      "learning_rate": 3.8709677419354835e-05,
      "loss": 1.447,
      "step": 6690
    },
    {
      "epoch": 1.7484342379958246,
      "grad_norm": 1.1004139184951782,
      "learning_rate": 3.831306187202538e-05,
      "loss": 1.654,
      "step": 6700
    },
    {
      "epoch": 1.751043841336117,
      "grad_norm": 1.2970285415649414,
      "learning_rate": 3.7916446324695926e-05,
      "loss": 1.6692,
      "step": 6710
    },
    {
      "epoch": 1.7536534446764092,
      "grad_norm": 1.5127637386322021,
      "learning_rate": 3.7519830777366474e-05,
      "loss": 1.5666,
      "step": 6720
    },
    {
      "epoch": 1.7562630480167014,
      "grad_norm": 1.4765899181365967,
      "learning_rate": 3.7123215230037016e-05,
      "loss": 1.7729,
      "step": 6730
    },
    {
      "epoch": 1.7588726513569939,
      "grad_norm": 1.0851900577545166,
      "learning_rate": 3.672659968270756e-05,
      "loss": 1.6621,
      "step": 6740
    },
    {
      "epoch": 1.761482254697286,
      "grad_norm": 1.3683584928512573,
      "learning_rate": 3.632998413537811e-05,
      "loss": 1.7928,
      "step": 6750
    },
    {
      "epoch": 1.7640918580375784,
      "grad_norm": 1.7666800022125244,
      "learning_rate": 3.593336858804865e-05,
      "loss": 1.7333,
      "step": 6760
    },
    {
      "epoch": 1.7667014613778704,
      "grad_norm": 1.4022432565689087,
      "learning_rate": 3.553675304071919e-05,
      "loss": 1.7736,
      "step": 6770
    },
    {
      "epoch": 1.769311064718163,
      "grad_norm": 1.462194561958313,
      "learning_rate": 3.514013749338974e-05,
      "loss": 1.5318,
      "step": 6780
    },
    {
      "epoch": 1.7719206680584552,
      "grad_norm": 1.3313997983932495,
      "learning_rate": 3.474352194606028e-05,
      "loss": 1.7717,
      "step": 6790
    },
    {
      "epoch": 1.7745302713987474,
      "grad_norm": 2.302496910095215,
      "learning_rate": 3.4346906398730824e-05,
      "loss": 1.6464,
      "step": 6800
    },
    {
      "epoch": 1.7745302713987474,
      "eval_loss": 1.6391409635543823,
      "eval_runtime": 1.9561,
      "eval_samples_per_second": 61.346,
      "eval_steps_per_second": 7.668,
      "step": 6800
    },
    {
      "epoch": 1.7771398747390397,
      "grad_norm": 1.7339266538619995,
      "learning_rate": 3.395029085140137e-05,
      "loss": 1.5224,
      "step": 6810
    },
    {
      "epoch": 1.779749478079332,
      "grad_norm": 1.647902250289917,
      "learning_rate": 3.355367530407192e-05,
      "loss": 1.5624,
      "step": 6820
    },
    {
      "epoch": 1.7823590814196242,
      "grad_norm": 1.6276928186416626,
      "learning_rate": 3.315705975674246e-05,
      "loss": 1.6941,
      "step": 6830
    },
    {
      "epoch": 1.7849686847599164,
      "grad_norm": 1.8065013885498047,
      "learning_rate": 3.2760444209413005e-05,
      "loss": 1.6791,
      "step": 6840
    },
    {
      "epoch": 1.787578288100209,
      "grad_norm": 1.7411856651306152,
      "learning_rate": 3.2363828662083554e-05,
      "loss": 1.5074,
      "step": 6850
    },
    {
      "epoch": 1.790187891440501,
      "grad_norm": 1.128658652305603,
      "learning_rate": 3.1967213114754096e-05,
      "loss": 1.3368,
      "step": 6860
    },
    {
      "epoch": 1.7927974947807934,
      "grad_norm": 1.2424094676971436,
      "learning_rate": 3.157059756742464e-05,
      "loss": 1.3328,
      "step": 6870
    },
    {
      "epoch": 1.7954070981210855,
      "grad_norm": 1.3185994625091553,
      "learning_rate": 3.117398202009519e-05,
      "loss": 1.5166,
      "step": 6880
    },
    {
      "epoch": 1.798016701461378,
      "grad_norm": 0.9317311644554138,
      "learning_rate": 3.077736647276573e-05,
      "loss": 1.6755,
      "step": 6890
    },
    {
      "epoch": 1.8006263048016702,
      "grad_norm": 1.4653172492980957,
      "learning_rate": 3.0380750925436274e-05,
      "loss": 1.4651,
      "step": 6900
    },
    {
      "epoch": 1.8032359081419624,
      "grad_norm": 1.0153963565826416,
      "learning_rate": 2.998413537810682e-05,
      "loss": 1.492,
      "step": 6910
    },
    {
      "epoch": 1.8058455114822547,
      "grad_norm": 1.6642237901687622,
      "learning_rate": 2.9587519830777365e-05,
      "loss": 1.634,
      "step": 6920
    },
    {
      "epoch": 1.808455114822547,
      "grad_norm": 2.018538236618042,
      "learning_rate": 2.919090428344791e-05,
      "loss": 1.6112,
      "step": 6930
    },
    {
      "epoch": 1.8110647181628392,
      "grad_norm": 1.5742379426956177,
      "learning_rate": 2.8794288736118452e-05,
      "loss": 1.5421,
      "step": 6940
    },
    {
      "epoch": 1.8136743215031315,
      "grad_norm": 1.1579594612121582,
      "learning_rate": 2.8397673188788997e-05,
      "loss": 1.6379,
      "step": 6950
    },
    {
      "epoch": 1.816283924843424,
      "grad_norm": 1.5624881982803345,
      "learning_rate": 2.8001057641459543e-05,
      "loss": 1.5502,
      "step": 6960
    },
    {
      "epoch": 1.818893528183716,
      "grad_norm": 1.4905972480773926,
      "learning_rate": 2.7604442094130085e-05,
      "loss": 1.3638,
      "step": 6970
    },
    {
      "epoch": 1.8215031315240084,
      "grad_norm": 1.4103835821151733,
      "learning_rate": 2.7207826546800633e-05,
      "loss": 1.4859,
      "step": 6980
    },
    {
      "epoch": 1.8241127348643005,
      "grad_norm": 1.341662049293518,
      "learning_rate": 2.681121099947118e-05,
      "loss": 1.6178,
      "step": 6990
    },
    {
      "epoch": 1.826722338204593,
      "grad_norm": 1.5594878196716309,
      "learning_rate": 2.6414595452141724e-05,
      "loss": 1.4909,
      "step": 7000
    },
    {
      "epoch": 1.826722338204593,
      "eval_loss": 1.6380412578582764,
      "eval_runtime": 1.9581,
      "eval_samples_per_second": 61.285,
      "eval_steps_per_second": 7.661,
      "step": 7000
    },
    {
      "epoch": 1.8293319415448852,
      "grad_norm": 1.1249526739120483,
      "learning_rate": 2.6017979904812266e-05,
      "loss": 1.4015,
      "step": 7010
    },
    {
      "epoch": 1.8319415448851775,
      "grad_norm": 1.2737579345703125,
      "learning_rate": 2.562136435748281e-05,
      "loss": 1.4153,
      "step": 7020
    },
    {
      "epoch": 1.8345511482254697,
      "grad_norm": 1.3299663066864014,
      "learning_rate": 2.5224748810153357e-05,
      "loss": 1.8845,
      "step": 7030
    },
    {
      "epoch": 1.837160751565762,
      "grad_norm": 1.4723020792007446,
      "learning_rate": 2.48281332628239e-05,
      "loss": 1.6985,
      "step": 7040
    },
    {
      "epoch": 1.8397703549060542,
      "grad_norm": 1.4264949560165405,
      "learning_rate": 2.4431517715494444e-05,
      "loss": 1.8354,
      "step": 7050
    },
    {
      "epoch": 1.8423799582463465,
      "grad_norm": 1.194126844406128,
      "learning_rate": 2.403490216816499e-05,
      "loss": 1.6229,
      "step": 7060
    },
    {
      "epoch": 1.844989561586639,
      "grad_norm": 1.537436604499817,
      "learning_rate": 2.3638286620835535e-05,
      "loss": 1.411,
      "step": 7070
    },
    {
      "epoch": 1.847599164926931,
      "grad_norm": 1.3387829065322876,
      "learning_rate": 2.3241671073506077e-05,
      "loss": 1.5275,
      "step": 7080
    },
    {
      "epoch": 1.8502087682672235,
      "grad_norm": 1.5078318119049072,
      "learning_rate": 2.2845055526176626e-05,
      "loss": 1.6097,
      "step": 7090
    },
    {
      "epoch": 1.8528183716075155,
      "grad_norm": 1.240625023841858,
      "learning_rate": 2.244843997884717e-05,
      "loss": 1.4366,
      "step": 7100
    },
    {
      "epoch": 1.855427974947808,
      "grad_norm": 1.1749022006988525,
      "learning_rate": 2.2051824431517713e-05,
      "loss": 1.5245,
      "step": 7110
    },
    {
      "epoch": 1.8580375782881002,
      "grad_norm": 1.5600993633270264,
      "learning_rate": 2.165520888418826e-05,
      "loss": 1.493,
      "step": 7120
    },
    {
      "epoch": 1.8606471816283925,
      "grad_norm": 1.3330864906311035,
      "learning_rate": 2.1258593336858804e-05,
      "loss": 1.5973,
      "step": 7130
    },
    {
      "epoch": 1.8632567849686847,
      "grad_norm": 1.5972740650177002,
      "learning_rate": 2.086197778952935e-05,
      "loss": 1.6879,
      "step": 7140
    },
    {
      "epoch": 1.865866388308977,
      "grad_norm": 1.2211631536483765,
      "learning_rate": 2.046536224219989e-05,
      "loss": 1.579,
      "step": 7150
    },
    {
      "epoch": 1.8684759916492695,
      "grad_norm": 1.9304025173187256,
      "learning_rate": 2.0068746694870436e-05,
      "loss": 1.4082,
      "step": 7160
    },
    {
      "epoch": 1.8710855949895615,
      "grad_norm": 1.6759915351867676,
      "learning_rate": 1.9672131147540982e-05,
      "loss": 1.4341,
      "step": 7170
    },
    {
      "epoch": 1.873695198329854,
      "grad_norm": 1.30782151222229,
      "learning_rate": 1.9275515600211524e-05,
      "loss": 1.4501,
      "step": 7180
    },
    {
      "epoch": 1.876304801670146,
      "grad_norm": 1.1257997751235962,
      "learning_rate": 1.887890005288207e-05,
      "loss": 1.5268,
      "step": 7190
    },
    {
      "epoch": 1.8789144050104385,
      "grad_norm": 1.5276806354522705,
      "learning_rate": 1.8482284505552618e-05,
      "loss": 1.5667,
      "step": 7200
    },
    {
      "epoch": 1.8789144050104385,
      "eval_loss": 1.6385174989700317,
      "eval_runtime": 1.9605,
      "eval_samples_per_second": 61.208,
      "eval_steps_per_second": 7.651,
      "step": 7200
    },
    {
      "epoch": 1.8815240083507305,
      "grad_norm": 1.3811612129211426,
      "learning_rate": 1.808566895822316e-05,
      "loss": 1.719,
      "step": 7210
    },
    {
      "epoch": 1.884133611691023,
      "grad_norm": 1.2322075366973877,
      "learning_rate": 1.7689053410893705e-05,
      "loss": 1.7173,
      "step": 7220
    },
    {
      "epoch": 1.8867432150313153,
      "grad_norm": 1.1626989841461182,
      "learning_rate": 1.729243786356425e-05,
      "loss": 1.6538,
      "step": 7230
    },
    {
      "epoch": 1.8893528183716075,
      "grad_norm": 1.158789038658142,
      "learning_rate": 1.6895822316234796e-05,
      "loss": 1.7137,
      "step": 7240
    },
    {
      "epoch": 1.8919624217118998,
      "grad_norm": 1.6415984630584717,
      "learning_rate": 1.6499206768905338e-05,
      "loss": 1.4713,
      "step": 7250
    },
    {
      "epoch": 1.894572025052192,
      "grad_norm": 1.6409275531768799,
      "learning_rate": 1.6102591221575883e-05,
      "loss": 1.5923,
      "step": 7260
    },
    {
      "epoch": 1.8971816283924845,
      "grad_norm": 1.5043424367904663,
      "learning_rate": 1.570597567424643e-05,
      "loss": 1.3784,
      "step": 7270
    },
    {
      "epoch": 1.8997912317327765,
      "grad_norm": 1.3688416481018066,
      "learning_rate": 1.5309360126916974e-05,
      "loss": 1.6791,
      "step": 7280
    },
    {
      "epoch": 1.902400835073069,
      "grad_norm": 1.1868476867675781,
      "learning_rate": 1.491274457958752e-05,
      "loss": 1.5863,
      "step": 7290
    },
    {
      "epoch": 1.905010438413361,
      "grad_norm": 1.159595012664795,
      "learning_rate": 1.4516129032258063e-05,
      "loss": 1.6496,
      "step": 7300
    },
    {
      "epoch": 1.9076200417536535,
      "grad_norm": 1.688033938407898,
      "learning_rate": 1.4119513484928608e-05,
      "loss": 1.597,
      "step": 7310
    },
    {
      "epoch": 1.9102296450939458,
      "grad_norm": 1.4096814393997192,
      "learning_rate": 1.3722897937599152e-05,
      "loss": 1.4939,
      "step": 7320
    },
    {
      "epoch": 1.912839248434238,
      "grad_norm": 1.3153157234191895,
      "learning_rate": 1.33262823902697e-05,
      "loss": 1.253,
      "step": 7330
    },
    {
      "epoch": 1.9154488517745303,
      "grad_norm": 1.4475085735321045,
      "learning_rate": 1.2929666842940243e-05,
      "loss": 1.7102,
      "step": 7340
    },
    {
      "epoch": 1.9180584551148225,
      "grad_norm": 1.1641416549682617,
      "learning_rate": 1.2533051295610787e-05,
      "loss": 1.6373,
      "step": 7350
    },
    {
      "epoch": 1.9206680584551148,
      "grad_norm": 0.9782329201698303,
      "learning_rate": 1.2136435748281332e-05,
      "loss": 1.7383,
      "step": 7360
    },
    {
      "epoch": 1.923277661795407,
      "grad_norm": 1.9701673984527588,
      "learning_rate": 1.1739820200951876e-05,
      "loss": 1.5786,
      "step": 7370
    },
    {
      "epoch": 1.9258872651356995,
      "grad_norm": 2.2239952087402344,
      "learning_rate": 1.1343204653622421e-05,
      "loss": 1.7553,
      "step": 7380
    },
    {
      "epoch": 1.9284968684759916,
      "grad_norm": 1.6950441598892212,
      "learning_rate": 1.0946589106292966e-05,
      "loss": 1.6305,
      "step": 7390
    },
    {
      "epoch": 1.931106471816284,
      "grad_norm": 1.5621709823608398,
      "learning_rate": 1.0549973558963512e-05,
      "loss": 1.6897,
      "step": 7400
    },
    {
      "epoch": 1.931106471816284,
      "eval_loss": 1.6360142230987549,
      "eval_runtime": 1.9574,
      "eval_samples_per_second": 61.306,
      "eval_steps_per_second": 7.663,
      "step": 7400
    },
    {
      "epoch": 1.933716075156576,
      "grad_norm": 1.6024967432022095,
      "learning_rate": 1.0153358011634055e-05,
      "loss": 1.5223,
      "step": 7410
    },
    {
      "epoch": 1.9363256784968685,
      "grad_norm": 1.7370519638061523,
      "learning_rate": 9.756742464304599e-06,
      "loss": 1.5561,
      "step": 7420
    },
    {
      "epoch": 1.9389352818371608,
      "grad_norm": 1.0128166675567627,
      "learning_rate": 9.360126916975144e-06,
      "loss": 1.6065,
      "step": 7430
    },
    {
      "epoch": 1.941544885177453,
      "grad_norm": 1.4676117897033691,
      "learning_rate": 8.96351136964569e-06,
      "loss": 1.6026,
      "step": 7440
    },
    {
      "epoch": 1.9441544885177453,
      "grad_norm": 1.959303617477417,
      "learning_rate": 8.566895822316235e-06,
      "loss": 1.5684,
      "step": 7450
    },
    {
      "epoch": 1.9467640918580376,
      "grad_norm": 1.1749241352081299,
      "learning_rate": 8.170280274986779e-06,
      "loss": 1.8297,
      "step": 7460
    },
    {
      "epoch": 1.9493736951983298,
      "grad_norm": 1.1877551078796387,
      "learning_rate": 7.773664727657322e-06,
      "loss": 1.7845,
      "step": 7470
    },
    {
      "epoch": 1.951983298538622,
      "grad_norm": 1.2506390810012817,
      "learning_rate": 7.377049180327868e-06,
      "loss": 1.5377,
      "step": 7480
    },
    {
      "epoch": 1.9545929018789145,
      "grad_norm": 1.1524018049240112,
      "learning_rate": 6.980433632998413e-06,
      "loss": 1.4858,
      "step": 7490
    },
    {
      "epoch": 1.9572025052192066,
      "grad_norm": 1.407309889793396,
      "learning_rate": 6.583818085668958e-06,
      "loss": 1.5745,
      "step": 7500
    },
    {
      "epoch": 1.959812108559499,
      "grad_norm": 1.2538046836853027,
      "learning_rate": 6.187202538339503e-06,
      "loss": 1.3688,
      "step": 7510
    },
    {
      "epoch": 1.962421711899791,
      "grad_norm": 0.8872684836387634,
      "learning_rate": 5.7905869910100475e-06,
      "loss": 1.4248,
      "step": 7520
    },
    {
      "epoch": 1.9650313152400836,
      "grad_norm": 1.505766749382019,
      "learning_rate": 5.393971443680592e-06,
      "loss": 1.7437,
      "step": 7530
    },
    {
      "epoch": 1.9676409185803758,
      "grad_norm": 1.351041555404663,
      "learning_rate": 4.9973558963511366e-06,
      "loss": 1.4078,
      "step": 7540
    },
    {
      "epoch": 1.970250521920668,
      "grad_norm": 2.100975275039673,
      "learning_rate": 4.600740349021682e-06,
      "loss": 1.6287,
      "step": 7550
    },
    {
      "epoch": 1.9728601252609603,
      "grad_norm": 1.5579477548599243,
      "learning_rate": 4.204124801692226e-06,
      "loss": 1.883,
      "step": 7560
    },
    {
      "epoch": 1.9754697286012526,
      "grad_norm": 1.1739943027496338,
      "learning_rate": 3.8075092543627705e-06,
      "loss": 1.7116,
      "step": 7570
    },
    {
      "epoch": 1.9780793319415448,
      "grad_norm": 1.426810622215271,
      "learning_rate": 3.4108937070333155e-06,
      "loss": 1.6511,
      "step": 7580
    },
    {
      "epoch": 1.980688935281837,
      "grad_norm": 1.2218469381332397,
      "learning_rate": 3.0142781597038604e-06,
      "loss": 1.4279,
      "step": 7590
    },
    {
      "epoch": 1.9832985386221296,
      "grad_norm": 1.60007643699646,
      "learning_rate": 2.617662612374405e-06,
      "loss": 1.7582,
      "step": 7600
    },
    {
      "epoch": 1.9832985386221296,
      "eval_loss": 1.63577139377594,
      "eval_runtime": 1.9575,
      "eval_samples_per_second": 61.302,
      "eval_steps_per_second": 7.663,
      "step": 7600
    }
  ],
  "logging_steps": 10,
  "max_steps": 7664,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.54050977654784e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
